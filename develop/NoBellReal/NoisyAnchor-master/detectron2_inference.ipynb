{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9614999e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import detectron2\n",
    "from detectron2.data import detection_utils as utils\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91b14f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register Dataset\n",
    "try:\n",
    "    register_coco_instances('coco_trash_test', {}, '../../../dataset/test.json', '../../../dataset/')\n",
    "except AssertionError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f7ad5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading config configs/COCO-Detection-NoisyAnchor/../Base-RetinaNet.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.\n"
     ]
    }
   ],
   "source": [
    "# config 불러오기\n",
    "cfg = get_cfg()\n",
    "# cfg.merge_from_file(model_zoo.get_config_file('COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml'))\n",
    "cfg.merge_from_file('configs/COCO-Detection-NoisyAnchor/retinanet_R_101_FPN_3x_noisyanchor.yaml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2259060b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config 수정하기\n",
    "cfg.DATASETS.TEST = ('coco_trash_test',)\n",
    "\n",
    "cfg.DATALOADER.NUM_WOREKRS = 2\n",
    "\n",
    "cfg.OUTPUT_DIR = './output'\n",
    "\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, 'model_final.pth') \n",
    "\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 10\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49e6aa74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[01/18 17:01:02 d2.data.datasets.coco]: \u001b[0mLoaded 4871 images in COCO format from ../../../dataset/test.json\n",
      "\u001b[32m[01/18 17:01:02 d2.data.common]: \u001b[0mSerializing 4871 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[01/18 17:01:02 d2.data.common]: \u001b[0mSerialized dataset takes 0.53 MiB\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "# mapper - input data를 어떤 형식으로 return할지\n",
    "def MyMapper(dataset_dict):\n",
    "    \n",
    "    dataset_dict = copy.deepcopy(dataset_dict)\n",
    "    image = utils.read_image(dataset_dict['file_name'], format='BGR')\n",
    "    \n",
    "    dataset_dict['image'] = image\n",
    "    \n",
    "    return dataset_dict\n",
    "\n",
    "# test loader\n",
    "test_loader = build_detection_test_loader(cfg, 'coco_trash_test', MyMapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da993da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/4871 [00:00<1:10:31,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved output image to: output/img/test/0000.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/4871 [00:01<1:04:53,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved output image to: output/img/test/0001.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/4871 [00:02<56:27,  1.44it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved output image to: output/img/test/0002.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/4871 [00:02<52:16,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved output image to: output/img/test/0003.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/4871 [00:03<49:47,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved output image to: output/img/test/0004.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/4871 [00:03<48:18,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved output image to: output/img/test/0005.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/4871 [00:04<51:36,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved output image to: output/img/test/0006.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/4871 [00:05<49:31,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved output image to: output/img/test/0007.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/4871 [00:05<48:00,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved output image to: output/img/test/0008.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/4871 [00:06<46:50,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved output image to: output/img/test/0009.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 11/4871 [00:06<51:09,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved output image to: output/img/test/0010.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m file_name \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../../dataset/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# 모델의 이미지에대한 예측 아웃풋\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstances\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     19\u001b[0m outputs_v \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Convert CUDA tensor to CPU tensor and then to numpy array\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# boxes = outputs.pred_boxes.tensor.detach().cpu().numpy()\u001b[39;00m\n",
      "File \u001b[0;32m~/level2-objectdetection-cv-10-1/develop/NoBellReal/NoisyAnchor-master/detectron2/engine/defaults.py:196\u001b[0m, in \u001b[0;36mDefaultPredictor.__call__\u001b[0;34m(self, original_image)\u001b[0m\n\u001b[1;32m    193\u001b[0m image \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(image\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    195\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m: image, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheight\u001b[39m\u001b[38;5;124m\"\u001b[39m: height, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m\"\u001b[39m: width}\n\u001b[0;32m--> 196\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/level2-objectdetection-cv-10-1/develop/NoBellReal/NoisyAnchor-master/detectron2/modeling/meta_arch/retinanet_noisy.py:230\u001b[0m, in \u001b[0;36mRetinaNetNoisy.forward\u001b[0;34m(self, batched_inputs)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m losses\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 230\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbox_cls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox_delta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manchors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m     processed_results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m results_per_image, input_per_image, image_size \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m    233\u001b[0m         results, batched_inputs, images\u001b[38;5;241m.\u001b[39mimage_sizes\n\u001b[1;32m    234\u001b[0m     ):\n",
      "File \u001b[0;32m~/level2-objectdetection-cv-10-1/develop/NoBellReal/NoisyAnchor-master/detectron2/modeling/meta_arch/retinanet_noisy.py:442\u001b[0m, in \u001b[0;36mRetinaNetNoisy.inference\u001b[0;34m(self, box_cls, box_delta, anchors, image_sizes)\u001b[0m\n\u001b[1;32m    440\u001b[0m     box_cls_per_image \u001b[38;5;241m=\u001b[39m [box_cls_per_level[img_idx] \u001b[38;5;28;01mfor\u001b[39;00m box_cls_per_level \u001b[38;5;129;01min\u001b[39;00m box_cls]\n\u001b[1;32m    441\u001b[0m     box_reg_per_image \u001b[38;5;241m=\u001b[39m [box_reg_per_level[img_idx] \u001b[38;5;28;01mfor\u001b[39;00m box_reg_per_level \u001b[38;5;129;01min\u001b[39;00m box_delta]\n\u001b[0;32m--> 442\u001b[0m     results_per_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference_single_image\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbox_cls_per_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox_reg_per_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manchors_per_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(results_per_image)\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m~/level2-objectdetection-cv-10-1/develop/NoBellReal/NoisyAnchor-master/detectron2/modeling/meta_arch/retinanet_noisy.py:490\u001b[0m, in \u001b[0;36mRetinaNetNoisy.inference_single_image\u001b[0;34m(self, box_cls, box_delta, anchors, image_size)\u001b[0m\n\u001b[1;32m    487\u001b[0m classes_idxs \u001b[38;5;241m=\u001b[39m topk_idxs \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes\n\u001b[1;32m    489\u001b[0m box_reg_i \u001b[38;5;241m=\u001b[39m box_reg_i[anchor_idxs]\n\u001b[0;32m--> 490\u001b[0m anchors_i \u001b[38;5;241m=\u001b[39m \u001b[43manchors_i\u001b[49m\u001b[43m[\u001b[49m\u001b[43manchor_idxs\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;66;03m# predict boxes\u001b[39;00m\n\u001b[1;32m    492\u001b[0m predicted_boxes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbox2box_transform\u001b[38;5;241m.\u001b[39mapply_deltas(box_reg_i, anchors_i\u001b[38;5;241m.\u001b[39mtensor)\n",
      "File \u001b[0;32m~/level2-objectdetection-cv-10-1/develop/NoBellReal/NoisyAnchor-master/detectron2/structures/boxes.py:208\u001b[0m, in \u001b[0;36mBoxes.__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    205\u001b[0m     keep \u001b[38;5;241m=\u001b[39m (widths \u001b[38;5;241m>\u001b[39m threshold) \u001b[38;5;241m&\u001b[39m (heights \u001b[38;5;241m>\u001b[39m threshold)\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m keep\n\u001b[0;32m--> 208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, item: Union[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mslice\u001b[39m, torch\u001b[38;5;241m.\u001b[39mBoolTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBoxes\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    209\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;124;03m        Boxes: Create a new :class:`Boxes` by indexing.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;124;03m    subject to Pytorch's indexing semantics.\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, \u001b[38;5;28mint\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# output 뽑은 후 sumbmission 양식에 맞게 후처리 \n",
    "prediction_strings = []\n",
    "file_names = []\n",
    "\n",
    "class_num = 10\n",
    "\n",
    "for data in tqdm(test_loader):\n",
    "    \n",
    "    prediction_string = ''\n",
    "    \n",
    "    data = data[0]\n",
    "    \n",
    "    # 출력용 이미지\n",
    "    image = data['image']\n",
    "    # 출력용 이미지 이름\n",
    "    file_name = data['file_name'].replace('../../../dataset/', '')\n",
    "    # 모델의 이미지에대한 예측 아웃풋\n",
    "    outputs = predictor(image)['instances']\n",
    "    outputs_v = outputs.to('cpu')\n",
    "    # Convert CUDA tensor to CPU tensor and then to numpy array\n",
    "    # boxes = outputs.pred_boxes.tensor.detach().cpu().numpy()\n",
    "\n",
    "    targets = outputs.pred_classes.cpu().tolist()\n",
    "    boxes = [i.cpu().detach().numpy() for i in outputs.pred_boxes]\n",
    "    scores = outputs.scores.cpu().tolist()\n",
    "    \n",
    "    # 출력용 이미지 파일 읽어와서 bbox 그리기\n",
    "    img = cv2.imread(data['file_name'])\n",
    "    v = Visualizer(img[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TEST[0]), scale=1.2)\n",
    "    v = v.draw_instance_predictions(outputs_v)\n",
    "    \n",
    "    # Save or display the image\n",
    "    visualized_img = v.get_image()[:, :, ::-1]\n",
    "    cv2.imwrite(f\"./output/img/{file_name}\", visualized_img)\n",
    "    # 저장된 이미지 파일의 경로 출력\n",
    "    print(f\"Saved output image to: output/img/{file_name}\")\n",
    "\n",
    "    # 이미지 출력\n",
    "    # plt.imshow(visualized_img)\n",
    "    # plt.axis('off')\n",
    "    # plt.show()\n",
    "\n",
    "    for target, box, score in zip(targets,boxes,scores):\n",
    "        prediction_string += (str(target) + ' ' + str(score) + ' ' + str(box[0]) + ' ' \n",
    "        + str(box[1]) + ' ' + str(box[2]) + ' ' + str(box[3]) + ' ')\n",
    "     \n",
    "    prediction_strings.append(prediction_string)\n",
    "    file_names.append(data['file_name'].replace('../../../dataset/',''))\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "submission['PredictionString'] = prediction_strings\n",
    "submission['image_id'] = file_names\n",
    "submission.to_csv(os.path.join(cfg.OUTPUT_DIR, f'submission_det2.csv'), index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d55e68a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4871/4871 [09:31<00:00,  8.52it/s]\n"
     ]
    }
   ],
   "source": [
    "# output 뽑은 후 sumbmission 양식에 맞게 후처리 \n",
    "prediction_strings = []\n",
    "file_names = []\n",
    "\n",
    "class_num = 10\n",
    "\n",
    "# 클래스별 고유한 색상 정의 (10개 클래스에 대한 색상)\n",
    "class_colors = [(0, 255, 0), (0, 0, 255), (255, 0, 0), (0, 255, 255), (255, 255, 0),\n",
    "                (255, 0, 255), (128, 0, 0), (0, 128, 0), (0, 0, 128), (128, 128, 128)]\n",
    "\n",
    "for data in tqdm(test_loader):\n",
    "    \n",
    "    prediction_string = ''\n",
    "    \n",
    "    data = data[0]\n",
    "    \n",
    "    # 출력용 이미지\n",
    "    image = data['image']\n",
    "    # 출력용 이미지 이름\n",
    "    file_name = data['file_name'].replace('../../../dataset/', '')\n",
    "    # 모델의 이미지에대한 예측 아웃풋\n",
    "    outputs = predictor(image)['instances']\n",
    "\n",
    "    targets = outputs.pred_classes.cpu().tolist()\n",
    "    boxes = [i.cpu().detach().numpy() for i in outputs.pred_boxes]\n",
    "    scores = outputs.scores.cpu().tolist()\n",
    "\n",
    "    # 선택할 신뢰도의 임계값 설정 (예: 0.5)\n",
    "    confidence_threshold = 0.2\n",
    "    \n",
    "    # 가장 높은 신뢰도의 bbox 선택\n",
    "    selected_boxes = []\n",
    "    for target, box, score in zip(targets, boxes, scores):\n",
    "        if score >= confidence_threshold:\n",
    "            selected_boxes.append((target, round(score, 2), box))\n",
    "\n",
    "    # 신뢰도를 기준으로 내림차순으로 정렬\n",
    "    selected_boxes.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    img = cv2.imread(data['file_name'])\n",
    "\n",
    "    for target, score, box in selected_boxes:\n",
    "        prediction_string += (str(target) + ' ' + str(score) + ' ' + str(box[0]) + ' ' \n",
    "        + str(box[1]) + ' ' + str(box[2]) + ' ' + str(box[3]) + ' ')\n",
    "\n",
    "        # Draw bounding box\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        class_color = class_colors[target % len(class_colors)]\n",
    "        thickness = 2\n",
    "        img = cv2.rectangle(img, (x1, y1), (x2, y2), class_color, thickness)\n",
    "        \n",
    "        # Label text\n",
    "        class_name = str(target)\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        font_scale = 0.5\n",
    "        font_thickness = 1\n",
    "        text_size = cv2.getTextSize(class_name, font, font_scale, font_thickness)[0]\n",
    "        img = cv2.putText(img, class_name, (x1, y1 - 5), font, font_scale, class_color, font_thickness)\n",
    "    \n",
    "    cv2.imwrite(f\"./output/img/{file_name}\", img)\n",
    "    # 저장된 이미지 파일의 경로 출력\n",
    "    # print(f\"Saved output image to: output/img/{file_name}\")\n",
    "\n",
    "    # # 이미지 출력\n",
    "    # plt.imshow(img)\n",
    "    # plt.axis('off')\n",
    "    # plt.show()\n",
    "    # break\n",
    "    prediction_strings.append(prediction_string)\n",
    "    file_names.append(data['file_name'].replace('../../../dataset/',''))\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "submission['PredictionString'] = prediction_strings\n",
    "submission['image_id'] = file_names\n",
    "submission.to_csv(os.path.join(cfg.OUTPUT_DIR, f'submission_det2.csv'), index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f947a34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
