{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run only on the first execution\n",
    "# !python setup.py build develop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import detectron2\n",
    "from detectron2.data import detection_utils as utils\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.evaluation import COCOEvaluator\n",
    "from detectron2.data import build_detection_test_loader, build_detection_train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register Dataset\n",
    "try:\n",
    "    register_coco_instances('coco_trash_train', {}, '../../../dataset/train.json', '../../../dataset/')\n",
    "except AssertionError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    register_coco_instances('coco_trash_test', {}, '../../../dataset/test.json', '../../../dataset/')\n",
    "except AssertionError:\n",
    "    pass\n",
    "\n",
    "MetadataCatalog.get('coco_trash_train').thing_classes = [\"General trash\", \"Paper\", \"Paper pack\", \"Metal\", \n",
    "                                                         \"Glass\", \"Plastic\", \"Styrofoam\", \"Plastic bag\", \"Battery\", \"Clothing\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config 불러오기\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file('COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config 수정하기\n",
    "cfg.DATASETS.TRAIN = ('coco_trash_train',)\n",
    "cfg.DATASETS.TEST = ('coco_trash_test',)\n",
    "\n",
    "cfg.DATALOADER.NUM_WOREKRS = 0\n",
    "\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url('COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml')\n",
    "\n",
    "cfg.SOLVER.IMS_PER_BATCH = 16\n",
    "cfg.SOLVER.BASE_LR = 0.001\n",
    "cfg.SOLVER.MAX_ITER = 15000\n",
    "cfg.SOLVER.STEPS = (8000,12000)\n",
    "cfg.SOLVER.GAMMA = 0.005\n",
    "cfg.SOLVER.CHECKPOINT_PERIOD = 3000\n",
    "\n",
    "cfg.OUTPUT_DIR = './output'\n",
    "\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 12\n",
    "\n",
    "cfg.TEST.EVAL_PERIOD = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapper - input data를 어떤 형식으로 return할지 (따라서 augmnentation 등 데이터 전처리 포함 됨)\n",
    "import detectron2.data.transforms as T\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "def MyMapper(dataset_dict):\n",
    "    dataset_dict = copy.deepcopy(dataset_dict)\n",
    "    image = utils.read_image(dataset_dict['file_name'], format='BGR')\n",
    "    \n",
    "    transform_list = [\n",
    "        T.RandomFlip(prob=0.5, horizontal=False, vertical=True),\n",
    "        T.RandomFlip(prob=0.5, horizontal=True, vertical=False),\n",
    "        T.RandomBrightness(0.8, 1.5),\n",
    "        # T.RandomContrast(0.6, 1.3),\n",
    "        T.Resize(1024)\n",
    "        # A.Resize(1024, 1024),\n",
    "        # ToTensorV2(p=0.1)\n",
    "\n",
    "    ]\n",
    "    \n",
    "    image, transforms = T.apply_transform_gens(transform_list, image)\n",
    "    \n",
    "    dataset_dict['image'] = torch.as_tensor(image.transpose(2,0,1).astype('float32'))\n",
    "    \n",
    "    annos = [\n",
    "        utils.transform_instance_annotations(obj, transforms, image.shape[:2])\n",
    "        for obj in dataset_dict.pop('annotations')\n",
    "        if obj.get('iscrowd', 0) == 0\n",
    "    ]\n",
    "    \n",
    "    instances = utils.annotations_to_instances(annos, image.shape[:2])\n",
    "    dataset_dict['instances'] = utils.filter_empty_instances(instances)\n",
    "    \n",
    "    return dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer - DefaultTrainer를 상속\n",
    "class MyTrainer(DefaultTrainer):\n",
    "    \n",
    "    @classmethod\n",
    "    def build_train_loader(cls, cfg, sampler=None):\n",
    "        return build_detection_train_loader(\n",
    "        cfg, mapper = MyMapper, sampler = sampler\n",
    "        )\n",
    "    \n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
    "        if output_folder is None:\n",
    "            os.makedirs('./output_eval', exist_ok = True)\n",
    "            output_folder = './output_eval'\n",
    "            \n",
    "        return COCOEvaluator(dataset_name, cfg, False, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok = True)\n",
    "\n",
    "trainer = MyTrainer(cfg)\n",
    "trainer.resume_or_load(resume=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[01/08 15:45:34 d2.engine.defaults]: \u001b[0mModel:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (6): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (7): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (8): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (9): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (10): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (11): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (12): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (13): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (14): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (15): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (16): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (17): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (18): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (19): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (20): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (21): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (22): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=13, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=48, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m[01/08 15:45:34 d2.data.datasets.coco]: \u001b[0mLoaded 4883 images in COCO format from ../../../dataset/train.json\n",
      "\u001b[32m[01/08 15:45:34 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 4883 images left.\n",
      "\u001b[32m[01/08 15:45:34 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
      "\u001b[32m[01/08 15:45:34 d2.data.common]: \u001b[0mSerializing 4883 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[01/08 15:45:34 d2.data.common]: \u001b[0mSerialized dataset takes 2.20 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (13, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (13,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (48, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (48,) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[01/08 15:45:35 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ephemeral/home/level2-objectdetection-cv-10/develop/StringZero/detectron2/detectron2/structures/boxes.py:158: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  tensor = torch.as_tensor(tensor, dtype=torch.float32, device=device)\n",
      "/data/ephemeral/home/level2-objectdetection-cv-10/develop/StringZero/detectron2/detectron2/structures/boxes.py:158: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  tensor = torch.as_tensor(tensor, dtype=torch.float32, device=device)\n",
      "/data/ephemeral/home/level2-objectdetection-cv-10/develop/StringZero/detectron2/detectron2/structures/boxes.py:158: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  tensor = torch.as_tensor(tensor, dtype=torch.float32, device=device)\n",
      "/data/ephemeral/home/level2-objectdetection-cv-10/develop/StringZero/detectron2/detectron2/structures/boxes.py:158: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  tensor = torch.as_tensor(tensor, dtype=torch.float32, device=device)\n",
      "/data/ephemeral/home/level2-objectdetection-cv-10/develop/StringZero/detectron2/detectron2/structures/image_list.py:99: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  max_size = (max_size + (stride - 1)) // stride * stride\n",
      "/opt/conda/lib/python3.10/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[01/08 15:46:14 d2.utils.events]: \u001b[0m eta: 7:49:05  iter: 19  total_loss: 3.394  loss_cls: 2.553  loss_box_reg: 0.6921  loss_rpn_cls: 0.1043  loss_rpn_loc: 0.03723  time: 1.8812  data_time: 0.2131  lr: 1.9981e-05  max_mem: 24877M\n",
      "\u001b[32m[01/08 15:46:52 d2.utils.events]: \u001b[0m eta: 7:48:14  iter: 39  total_loss: 3.014  loss_cls: 2.102  loss_box_reg: 0.6915  loss_rpn_cls: 0.1026  loss_rpn_loc: 0.04886  time: 1.8811  data_time: 0.2006  lr: 3.9961e-05  max_mem: 24877M\n",
      "\u001b[32m[01/08 15:47:30 d2.utils.events]: \u001b[0m eta: 7:47:30  iter: 59  total_loss: 2.15  loss_cls: 1.28  loss_box_reg: 0.6844  loss_rpn_cls: 0.08379  loss_rpn_loc: 0.03134  time: 1.8796  data_time: 0.1960  lr: 5.9941e-05  max_mem: 24877M\n",
      "\u001b[32m[01/08 15:48:07 d2.utils.events]: \u001b[0m eta: 7:46:40  iter: 79  total_loss: 1.682  loss_cls: 0.8397  loss_box_reg: 0.6914  loss_rpn_cls: 0.1109  loss_rpn_loc: 0.03811  time: 1.8796  data_time: 0.1971  lr: 7.9921e-05  max_mem: 24877M\n",
      "\u001b[32m[01/08 15:48:45 d2.utils.events]: \u001b[0m eta: 7:46:05  iter: 99  total_loss: 1.557  loss_cls: 0.7595  loss_box_reg: 0.6968  loss_rpn_cls: 0.06614  loss_rpn_loc: 0.03549  time: 1.8815  data_time: 0.2077  lr: 9.9901e-05  max_mem: 24877M\n",
      "\u001b[32m[01/08 15:49:23 d2.utils.events]: \u001b[0m eta: 7:45:31  iter: 119  total_loss: 1.644  loss_cls: 0.7756  loss_box_reg: 0.7462  loss_rpn_cls: 0.05621  loss_rpn_loc: 0.04286  time: 1.8822  data_time: 0.2015  lr: 0.00011988  max_mem: 24877M\n",
      "\u001b[32m[01/08 15:50:00 d2.utils.events]: \u001b[0m eta: 7:44:51  iter: 139  total_loss: 1.531  loss_cls: 0.7343  loss_box_reg: 0.714  loss_rpn_cls: 0.04742  loss_rpn_loc: 0.02931  time: 1.8819  data_time: 0.2002  lr: 0.00013986  max_mem: 24877M\n",
      "\u001b[32m[01/08 15:50:38 d2.utils.events]: \u001b[0m eta: 7:44:28  iter: 159  total_loss: 1.48  loss_cls: 0.6868  loss_box_reg: 0.6848  loss_rpn_cls: 0.04994  loss_rpn_loc: 0.03735  time: 1.8824  data_time: 0.2023  lr: 0.00015984  max_mem: 24877M\n",
      "\u001b[32m[01/08 15:51:16 d2.utils.events]: \u001b[0m eta: 7:43:46  iter: 179  total_loss: 1.421  loss_cls: 0.6632  loss_box_reg: 0.6905  loss_rpn_cls: 0.03166  loss_rpn_loc: 0.03512  time: 1.8820  data_time: 0.1954  lr: 0.00017982  max_mem: 24877M\n",
      "\u001b[32m[01/08 15:51:53 d2.utils.events]: \u001b[0m eta: 7:43:17  iter: 199  total_loss: 1.513  loss_cls: 0.6805  loss_box_reg: 0.7224  loss_rpn_cls: 0.05535  loss_rpn_loc: 0.04496  time: 1.8822  data_time: 0.1998  lr: 0.0001998  max_mem: 24877M\n",
      "\u001b[32m[01/08 15:52:31 d2.utils.events]: \u001b[0m eta: 7:42:42  iter: 219  total_loss: 1.425  loss_cls: 0.6444  loss_box_reg: 0.6935  loss_rpn_cls: 0.04271  loss_rpn_loc: 0.03548  time: 1.8821  data_time: 0.1980  lr: 0.00021978  max_mem: 24877M\n",
      "\u001b[32m[01/08 15:53:09 d2.utils.events]: \u001b[0m eta: 7:42:04  iter: 239  total_loss: 1.316  loss_cls: 0.5847  loss_box_reg: 0.6621  loss_rpn_cls: 0.03072  loss_rpn_loc: 0.03431  time: 1.8818  data_time: 0.1993  lr: 0.00023976  max_mem: 24877M\n",
      "\u001b[32m[01/08 15:53:46 d2.utils.events]: \u001b[0m eta: 7:41:29  iter: 259  total_loss: 1.444  loss_cls: 0.6607  loss_box_reg: 0.7025  loss_rpn_cls: 0.04097  loss_rpn_loc: 0.03988  time: 1.8821  data_time: 0.2026  lr: 0.00025974  max_mem: 24877M\n",
      "\u001b[32m[01/08 15:54:24 d2.utils.events]: \u001b[0m eta: 7:40:51  iter: 279  total_loss: 1.382  loss_cls: 0.6305  loss_box_reg: 0.6636  loss_rpn_cls: 0.04073  loss_rpn_loc: 0.03684  time: 1.8820  data_time: 0.1979  lr: 0.00027972  max_mem: 24877M\n",
      "\u001b[32m[01/08 15:55:02 d2.utils.events]: \u001b[0m eta: 7:40:14  iter: 299  total_loss: 1.251  loss_cls: 0.5516  loss_box_reg: 0.6115  loss_rpn_cls: 0.02827  loss_rpn_loc: 0.03856  time: 1.8819  data_time: 0.2022  lr: 0.0002997  max_mem: 24877M\n",
      "\u001b[32m[01/08 15:55:39 d2.utils.events]: \u001b[0m eta: 7:39:42  iter: 319  total_loss: 1.238  loss_cls: 0.5679  loss_box_reg: 0.5799  loss_rpn_cls: 0.03524  loss_rpn_loc: 0.04343  time: 1.8824  data_time: 0.2076  lr: 0.00031968  max_mem: 24877M\n",
      "\u001b[32m[01/08 15:56:17 d2.utils.events]: \u001b[0m eta: 7:39:05  iter: 339  total_loss: 1.233  loss_cls: 0.5759  loss_box_reg: 0.569  loss_rpn_cls: 0.03932  loss_rpn_loc: 0.0367  time: 1.8825  data_time: 0.2025  lr: 0.00033966  max_mem: 24877M\n",
      "\u001b[32m[01/08 15:56:55 d2.utils.events]: \u001b[0m eta: 7:38:27  iter: 359  total_loss: 1.089  loss_cls: 0.531  loss_box_reg: 0.4801  loss_rpn_cls: 0.03709  loss_rpn_loc: 0.03501  time: 1.8826  data_time: 0.2007  lr: 0.00035964  max_mem: 24877M\n",
      "\u001b[32m[01/08 15:57:32 d2.utils.events]: \u001b[0m eta: 7:37:48  iter: 379  total_loss: 0.9586  loss_cls: 0.4851  loss_box_reg: 0.4161  loss_rpn_cls: 0.02636  loss_rpn_loc: 0.02947  time: 1.8824  data_time: 0.1953  lr: 0.00037962  max_mem: 24877M\n",
      "\u001b[32m[01/08 15:58:10 d2.utils.events]: \u001b[0m eta: 7:37:08  iter: 399  total_loss: 1.002  loss_cls: 0.4979  loss_box_reg: 0.4307  loss_rpn_cls: 0.0354  loss_rpn_loc: 0.04141  time: 1.8820  data_time: 0.1964  lr: 0.0003996  max_mem: 24877M\n",
      "\u001b[32m[01/08 15:58:48 d2.utils.events]: \u001b[0m eta: 7:36:29  iter: 419  total_loss: 1.05  loss_cls: 0.5251  loss_box_reg: 0.4225  loss_rpn_cls: 0.03053  loss_rpn_loc: 0.04157  time: 1.8819  data_time: 0.1972  lr: 0.00041958  max_mem: 24877M\n",
      "\u001b[32m[01/08 15:59:25 d2.utils.events]: \u001b[0m eta: 7:35:50  iter: 439  total_loss: 1.02  loss_cls: 0.5193  loss_box_reg: 0.4013  loss_rpn_cls: 0.03435  loss_rpn_loc: 0.04207  time: 1.8820  data_time: 0.2031  lr: 0.00043956  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:00:03 d2.utils.events]: \u001b[0m eta: 7:35:14  iter: 459  total_loss: 0.893  loss_cls: 0.4677  loss_box_reg: 0.3585  loss_rpn_cls: 0.03053  loss_rpn_loc: 0.03313  time: 1.8822  data_time: 0.2016  lr: 0.00045954  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:00:41 d2.utils.events]: \u001b[0m eta: 7:34:36  iter: 479  total_loss: 0.9309  loss_cls: 0.4878  loss_box_reg: 0.3671  loss_rpn_cls: 0.0312  loss_rpn_loc: 0.03335  time: 1.8822  data_time: 0.2006  lr: 0.00047952  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:01:18 d2.utils.events]: \u001b[0m eta: 7:34:03  iter: 499  total_loss: 0.9713  loss_cls: 0.5115  loss_box_reg: 0.3746  loss_rpn_cls: 0.03147  loss_rpn_loc: 0.04387  time: 1.8824  data_time: 0.2020  lr: 0.0004995  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:01:56 d2.utils.events]: \u001b[0m eta: 7:33:26  iter: 519  total_loss: 0.9077  loss_cls: 0.4819  loss_box_reg: 0.3604  loss_rpn_cls: 0.03447  loss_rpn_loc: 0.03488  time: 1.8825  data_time: 0.2009  lr: 0.00051948  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:02:34 d2.utils.events]: \u001b[0m eta: 7:32:47  iter: 539  total_loss: 0.9459  loss_cls: 0.4645  loss_box_reg: 0.3496  loss_rpn_cls: 0.03636  loss_rpn_loc: 0.04527  time: 1.8823  data_time: 0.1974  lr: 0.00053946  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:03:11 d2.utils.events]: \u001b[0m eta: 7:32:11  iter: 559  total_loss: 0.845  loss_cls: 0.4744  loss_box_reg: 0.3217  loss_rpn_cls: 0.02764  loss_rpn_loc: 0.03056  time: 1.8824  data_time: 0.2030  lr: 0.00055944  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:03:49 d2.utils.events]: \u001b[0m eta: 7:31:32  iter: 579  total_loss: 0.8537  loss_cls: 0.4599  loss_box_reg: 0.3345  loss_rpn_cls: 0.03319  loss_rpn_loc: 0.03994  time: 1.8824  data_time: 0.2019  lr: 0.00057942  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:04:27 d2.utils.events]: \u001b[0m eta: 7:30:54  iter: 599  total_loss: 0.9149  loss_cls: 0.4747  loss_box_reg: 0.3302  loss_rpn_cls: 0.03244  loss_rpn_loc: 0.0341  time: 1.8823  data_time: 0.2011  lr: 0.0005994  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:05:04 d2.utils.events]: \u001b[0m eta: 7:30:15  iter: 619  total_loss: 0.8189  loss_cls: 0.4544  loss_box_reg: 0.3099  loss_rpn_cls: 0.03045  loss_rpn_loc: 0.0308  time: 1.8823  data_time: 0.1994  lr: 0.00061938  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:05:42 d2.utils.events]: \u001b[0m eta: 7:29:39  iter: 639  total_loss: 0.8777  loss_cls: 0.4755  loss_box_reg: 0.331  loss_rpn_cls: 0.03617  loss_rpn_loc: 0.04867  time: 1.8824  data_time: 0.2029  lr: 0.00063936  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:06:20 d2.utils.events]: \u001b[0m eta: 7:29:02  iter: 659  total_loss: 0.8634  loss_cls: 0.4734  loss_box_reg: 0.3338  loss_rpn_cls: 0.03023  loss_rpn_loc: 0.03299  time: 1.8826  data_time: 0.2005  lr: 0.00065934  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:06:58 d2.utils.events]: \u001b[0m eta: 7:28:28  iter: 679  total_loss: 0.847  loss_cls: 0.4569  loss_box_reg: 0.3024  loss_rpn_cls: 0.03396  loss_rpn_loc: 0.02901  time: 1.8829  data_time: 0.2060  lr: 0.00067932  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:07:35 d2.utils.events]: \u001b[0m eta: 7:27:50  iter: 699  total_loss: 0.8495  loss_cls: 0.4467  loss_box_reg: 0.2895  loss_rpn_cls: 0.02829  loss_rpn_loc: 0.02994  time: 1.8828  data_time: 0.2011  lr: 0.0006993  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:08:13 d2.utils.events]: \u001b[0m eta: 7:27:12  iter: 719  total_loss: 0.8047  loss_cls: 0.439  loss_box_reg: 0.2893  loss_rpn_cls: 0.02315  loss_rpn_loc: 0.02665  time: 1.8828  data_time: 0.2014  lr: 0.00071928  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:08:51 d2.utils.events]: \u001b[0m eta: 7:26:34  iter: 739  total_loss: 0.8046  loss_cls: 0.4307  loss_box_reg: 0.2967  loss_rpn_cls: 0.02598  loss_rpn_loc: 0.0329  time: 1.8828  data_time: 0.1965  lr: 0.00073926  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:09:28 d2.utils.events]: \u001b[0m eta: 7:25:56  iter: 759  total_loss: 0.7772  loss_cls: 0.4315  loss_box_reg: 0.2912  loss_rpn_cls: 0.02531  loss_rpn_loc: 0.03778  time: 1.8827  data_time: 0.1992  lr: 0.00075924  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:10:06 d2.utils.events]: \u001b[0m eta: 7:25:20  iter: 779  total_loss: 0.7446  loss_cls: 0.4074  loss_box_reg: 0.2867  loss_rpn_cls: 0.02717  loss_rpn_loc: 0.02675  time: 1.8827  data_time: 0.1974  lr: 0.00077922  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:10:44 d2.utils.events]: \u001b[0m eta: 7:24:44  iter: 799  total_loss: 0.8437  loss_cls: 0.4529  loss_box_reg: 0.3079  loss_rpn_cls: 0.03337  loss_rpn_loc: 0.04334  time: 1.8829  data_time: 0.2016  lr: 0.0007992  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:11:21 d2.utils.events]: \u001b[0m eta: 7:24:06  iter: 819  total_loss: 0.8193  loss_cls: 0.4526  loss_box_reg: 0.3059  loss_rpn_cls: 0.02649  loss_rpn_loc: 0.03694  time: 1.8827  data_time: 0.1954  lr: 0.00081918  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:11:59 d2.utils.events]: \u001b[0m eta: 7:23:29  iter: 839  total_loss: 0.7436  loss_cls: 0.4116  loss_box_reg: 0.2572  loss_rpn_cls: 0.02457  loss_rpn_loc: 0.03867  time: 1.8830  data_time: 0.1998  lr: 0.00083916  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:12:37 d2.utils.events]: \u001b[0m eta: 7:22:54  iter: 859  total_loss: 0.7937  loss_cls: 0.4331  loss_box_reg: 0.3097  loss_rpn_cls: 0.02813  loss_rpn_loc: 0.03296  time: 1.8830  data_time: 0.2020  lr: 0.00085914  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:13:15 d2.utils.events]: \u001b[0m eta: 7:22:15  iter: 879  total_loss: 0.8459  loss_cls: 0.4571  loss_box_reg: 0.3182  loss_rpn_cls: 0.03271  loss_rpn_loc: 0.04223  time: 1.8830  data_time: 0.1994  lr: 0.00087912  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:13:52 d2.utils.events]: \u001b[0m eta: 7:21:38  iter: 899  total_loss: 0.7638  loss_cls: 0.4224  loss_box_reg: 0.2783  loss_rpn_cls: 0.02853  loss_rpn_loc: 0.03117  time: 1.8829  data_time: 0.1983  lr: 0.0008991  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:14:30 d2.utils.events]: \u001b[0m eta: 7:21:00  iter: 919  total_loss: 0.8178  loss_cls: 0.4389  loss_box_reg: 0.3054  loss_rpn_cls: 0.02488  loss_rpn_loc: 0.03314  time: 1.8828  data_time: 0.1952  lr: 0.00091908  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:15:07 d2.utils.events]: \u001b[0m eta: 7:20:23  iter: 939  total_loss: 0.7397  loss_cls: 0.4158  loss_box_reg: 0.2724  loss_rpn_cls: 0.02291  loss_rpn_loc: 0.03277  time: 1.8828  data_time: 0.1995  lr: 0.00093906  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:15:45 d2.utils.events]: \u001b[0m eta: 7:19:46  iter: 959  total_loss: 0.7537  loss_cls: 0.4124  loss_box_reg: 0.2894  loss_rpn_cls: 0.0291  loss_rpn_loc: 0.03436  time: 1.8827  data_time: 0.1991  lr: 0.00095904  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:16:23 d2.utils.events]: \u001b[0m eta: 7:19:10  iter: 979  total_loss: 0.7831  loss_cls: 0.4392  loss_box_reg: 0.292  loss_rpn_cls: 0.02995  loss_rpn_loc: 0.03183  time: 1.8829  data_time: 0.2093  lr: 0.00097902  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:17:01 d2.utils.events]: \u001b[0m eta: 7:18:35  iter: 999  total_loss: 0.8032  loss_cls: 0.4261  loss_box_reg: 0.2752  loss_rpn_cls: 0.03628  loss_rpn_loc: 0.04255  time: 1.8830  data_time: 0.2004  lr: 0.000999  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:17:38 d2.utils.events]: \u001b[0m eta: 7:17:58  iter: 1019  total_loss: 0.7913  loss_cls: 0.4101  loss_box_reg: 0.2888  loss_rpn_cls: 0.0277  loss_rpn_loc: 0.03661  time: 1.8830  data_time: 0.1985  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:18:16 d2.utils.events]: \u001b[0m eta: 7:17:21  iter: 1039  total_loss: 0.7418  loss_cls: 0.4193  loss_box_reg: 0.2708  loss_rpn_cls: 0.02253  loss_rpn_loc: 0.03829  time: 1.8830  data_time: 0.1996  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:18:54 d2.utils.events]: \u001b[0m eta: 7:16:44  iter: 1059  total_loss: 0.7102  loss_cls: 0.3901  loss_box_reg: 0.2612  loss_rpn_cls: 0.03579  loss_rpn_loc: 0.03552  time: 1.8830  data_time: 0.1985  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:19:31 d2.utils.events]: \u001b[0m eta: 7:16:10  iter: 1079  total_loss: 0.7957  loss_cls: 0.4459  loss_box_reg: 0.3018  loss_rpn_cls: 0.02604  loss_rpn_loc: 0.03013  time: 1.8830  data_time: 0.1997  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:20:09 d2.utils.events]: \u001b[0m eta: 7:15:30  iter: 1099  total_loss: 0.7674  loss_cls: 0.4184  loss_box_reg: 0.295  loss_rpn_cls: 0.02722  loss_rpn_loc: 0.03823  time: 1.8829  data_time: 0.1950  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:20:47 d2.utils.events]: \u001b[0m eta: 7:14:52  iter: 1119  total_loss: 0.7474  loss_cls: 0.3986  loss_box_reg: 0.2758  loss_rpn_cls: 0.02065  loss_rpn_loc: 0.03651  time: 1.8828  data_time: 0.1996  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:21:24 d2.utils.events]: \u001b[0m eta: 7:14:18  iter: 1139  total_loss: 0.7829  loss_cls: 0.4094  loss_box_reg: 0.2974  loss_rpn_cls: 0.02404  loss_rpn_loc: 0.04301  time: 1.8829  data_time: 0.2027  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:22:02 d2.utils.events]: \u001b[0m eta: 7:13:42  iter: 1159  total_loss: 0.7218  loss_cls: 0.417  loss_box_reg: 0.2566  loss_rpn_cls: 0.02057  loss_rpn_loc: 0.03223  time: 1.8830  data_time: 0.2069  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:22:40 d2.utils.events]: \u001b[0m eta: 7:13:06  iter: 1179  total_loss: 0.7366  loss_cls: 0.4115  loss_box_reg: 0.2527  loss_rpn_cls: 0.01878  loss_rpn_loc: 0.02698  time: 1.8831  data_time: 0.2047  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:23:18 d2.utils.events]: \u001b[0m eta: 7:12:27  iter: 1199  total_loss: 0.7433  loss_cls: 0.3809  loss_box_reg: 0.2764  loss_rpn_cls: 0.02098  loss_rpn_loc: 0.03049  time: 1.8831  data_time: 0.2028  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:23:55 d2.utils.events]: \u001b[0m eta: 7:11:50  iter: 1219  total_loss: 0.7574  loss_cls: 0.4072  loss_box_reg: 0.283  loss_rpn_cls: 0.0255  loss_rpn_loc: 0.03629  time: 1.8831  data_time: 0.2007  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:24:33 d2.utils.events]: \u001b[0m eta: 7:11:13  iter: 1239  total_loss: 0.7123  loss_cls: 0.3925  loss_box_reg: 0.2745  loss_rpn_cls: 0.02205  loss_rpn_loc: 0.03367  time: 1.8831  data_time: 0.1985  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:25:11 d2.utils.events]: \u001b[0m eta: 7:10:36  iter: 1259  total_loss: 0.6908  loss_cls: 0.3806  loss_box_reg: 0.2587  loss_rpn_cls: 0.02094  loss_rpn_loc: 0.02659  time: 1.8830  data_time: 0.2001  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:25:48 d2.utils.events]: \u001b[0m eta: 7:10:00  iter: 1279  total_loss: 0.7256  loss_cls: 0.3996  loss_box_reg: 0.26  loss_rpn_cls: 0.02195  loss_rpn_loc: 0.03934  time: 1.8831  data_time: 0.1991  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:26:26 d2.utils.events]: \u001b[0m eta: 7:09:23  iter: 1299  total_loss: 0.7116  loss_cls: 0.3813  loss_box_reg: 0.2743  loss_rpn_cls: 0.02769  loss_rpn_loc: 0.02876  time: 1.8832  data_time: 0.2043  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:27:04 d2.utils.events]: \u001b[0m eta: 7:08:44  iter: 1319  total_loss: 0.7095  loss_cls: 0.4044  loss_box_reg: 0.2699  loss_rpn_cls: 0.02686  loss_rpn_loc: 0.03212  time: 1.8832  data_time: 0.2012  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:27:42 d2.utils.events]: \u001b[0m eta: 7:08:08  iter: 1339  total_loss: 0.7  loss_cls: 0.3759  loss_box_reg: 0.2792  loss_rpn_cls: 0.0221  loss_rpn_loc: 0.03335  time: 1.8833  data_time: 0.2063  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:28:19 d2.utils.events]: \u001b[0m eta: 7:07:30  iter: 1359  total_loss: 0.7391  loss_cls: 0.3875  loss_box_reg: 0.2738  loss_rpn_cls: 0.02138  loss_rpn_loc: 0.0351  time: 1.8833  data_time: 0.2046  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:28:57 d2.utils.events]: \u001b[0m eta: 7:06:53  iter: 1379  total_loss: 0.7639  loss_cls: 0.4077  loss_box_reg: 0.2779  loss_rpn_cls: 0.027  loss_rpn_loc: 0.03839  time: 1.8833  data_time: 0.2006  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:29:35 d2.utils.events]: \u001b[0m eta: 7:06:15  iter: 1399  total_loss: 0.7001  loss_cls: 0.3843  loss_box_reg: 0.2645  loss_rpn_cls: 0.02528  loss_rpn_loc: 0.03498  time: 1.8833  data_time: 0.2014  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:30:12 d2.utils.events]: \u001b[0m eta: 7:05:39  iter: 1419  total_loss: 0.7149  loss_cls: 0.3687  loss_box_reg: 0.2818  loss_rpn_cls: 0.02211  loss_rpn_loc: 0.03156  time: 1.8833  data_time: 0.1979  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:30:50 d2.utils.events]: \u001b[0m eta: 7:05:02  iter: 1439  total_loss: 0.7585  loss_cls: 0.408  loss_box_reg: 0.2901  loss_rpn_cls: 0.02777  loss_rpn_loc: 0.04006  time: 1.8833  data_time: 0.1993  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:31:28 d2.utils.events]: \u001b[0m eta: 7:04:24  iter: 1459  total_loss: 0.7255  loss_cls: 0.3809  loss_box_reg: 0.2688  loss_rpn_cls: 0.02743  loss_rpn_loc: 0.03521  time: 1.8833  data_time: 0.1985  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:32:05 d2.utils.events]: \u001b[0m eta: 7:03:49  iter: 1479  total_loss: 0.6933  loss_cls: 0.3859  loss_box_reg: 0.2852  loss_rpn_cls: 0.02129  loss_rpn_loc: 0.03227  time: 1.8833  data_time: 0.1995  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:32:43 d2.utils.events]: \u001b[0m eta: 7:03:10  iter: 1499  total_loss: 0.69  loss_cls: 0.3663  loss_box_reg: 0.2593  loss_rpn_cls: 0.02009  loss_rpn_loc: 0.02958  time: 1.8832  data_time: 0.1973  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:33:21 d2.utils.events]: \u001b[0m eta: 7:02:31  iter: 1519  total_loss: 0.6596  loss_cls: 0.3721  loss_box_reg: 0.2424  loss_rpn_cls: 0.01805  loss_rpn_loc: 0.02801  time: 1.8833  data_time: 0.1964  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:33:58 d2.utils.events]: \u001b[0m eta: 7:01:56  iter: 1539  total_loss: 0.6498  loss_cls: 0.3673  loss_box_reg: 0.2524  loss_rpn_cls: 0.02214  loss_rpn_loc: 0.03792  time: 1.8833  data_time: 0.2022  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:34:36 d2.utils.events]: \u001b[0m eta: 7:01:20  iter: 1559  total_loss: 0.6998  loss_cls: 0.3716  loss_box_reg: 0.2567  loss_rpn_cls: 0.02807  loss_rpn_loc: 0.02988  time: 1.8834  data_time: 0.2107  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:35:14 d2.utils.events]: \u001b[0m eta: 7:00:43  iter: 1579  total_loss: 0.6896  loss_cls: 0.3757  loss_box_reg: 0.2523  loss_rpn_cls: 0.02161  loss_rpn_loc: 0.02821  time: 1.8834  data_time: 0.1992  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:35:52 d2.utils.events]: \u001b[0m eta: 7:00:09  iter: 1599  total_loss: 0.6828  loss_cls: 0.3623  loss_box_reg: 0.2527  loss_rpn_cls: 0.02352  loss_rpn_loc: 0.03501  time: 1.8835  data_time: 0.2031  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:36:29 d2.utils.events]: \u001b[0m eta: 6:59:36  iter: 1619  total_loss: 0.7017  loss_cls: 0.3717  loss_box_reg: 0.2595  loss_rpn_cls: 0.02027  loss_rpn_loc: 0.02927  time: 1.8835  data_time: 0.2006  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:37:07 d2.utils.events]: \u001b[0m eta: 6:58:59  iter: 1639  total_loss: 0.6819  loss_cls: 0.3707  loss_box_reg: 0.2586  loss_rpn_cls: 0.02072  loss_rpn_loc: 0.02534  time: 1.8835  data_time: 0.2044  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:37:45 d2.utils.events]: \u001b[0m eta: 6:58:21  iter: 1659  total_loss: 0.7361  loss_cls: 0.3864  loss_box_reg: 0.2829  loss_rpn_cls: 0.02634  loss_rpn_loc: 0.03319  time: 1.8836  data_time: 0.2057  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:38:23 d2.utils.events]: \u001b[0m eta: 6:57:40  iter: 1679  total_loss: 0.6465  loss_cls: 0.3717  loss_box_reg: 0.2399  loss_rpn_cls: 0.02063  loss_rpn_loc: 0.02384  time: 1.8836  data_time: 0.2019  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:39:00 d2.utils.events]: \u001b[0m eta: 6:57:05  iter: 1699  total_loss: 0.71  loss_cls: 0.3625  loss_box_reg: 0.263  loss_rpn_cls: 0.01928  loss_rpn_loc: 0.03561  time: 1.8837  data_time: 0.2021  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:39:38 d2.utils.events]: \u001b[0m eta: 6:56:29  iter: 1719  total_loss: 0.7462  loss_cls: 0.3925  loss_box_reg: 0.2804  loss_rpn_cls: 0.02722  loss_rpn_loc: 0.0364  time: 1.8837  data_time: 0.2049  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:40:16 d2.utils.events]: \u001b[0m eta: 6:55:54  iter: 1739  total_loss: 0.6898  loss_cls: 0.3655  loss_box_reg: 0.2615  loss_rpn_cls: 0.02295  loss_rpn_loc: 0.02832  time: 1.8838  data_time: 0.2044  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:40:54 d2.utils.events]: \u001b[0m eta: 6:55:16  iter: 1759  total_loss: 0.7014  loss_cls: 0.3706  loss_box_reg: 0.2699  loss_rpn_cls: 0.02487  loss_rpn_loc: 0.03459  time: 1.8838  data_time: 0.1982  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:41:31 d2.utils.events]: \u001b[0m eta: 6:54:38  iter: 1779  total_loss: 0.6819  loss_cls: 0.369  loss_box_reg: 0.2529  loss_rpn_cls: 0.02129  loss_rpn_loc: 0.033  time: 1.8838  data_time: 0.1935  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:42:09 d2.utils.events]: \u001b[0m eta: 6:53:56  iter: 1799  total_loss: 0.6312  loss_cls: 0.3319  loss_box_reg: 0.2521  loss_rpn_cls: 0.01868  loss_rpn_loc: 0.03178  time: 1.8838  data_time: 0.2034  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:42:47 d2.utils.events]: \u001b[0m eta: 6:53:24  iter: 1819  total_loss: 0.662  loss_cls: 0.3613  loss_box_reg: 0.2482  loss_rpn_cls: 0.01741  loss_rpn_loc: 0.02926  time: 1.8838  data_time: 0.2064  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:43:25 d2.utils.events]: \u001b[0m eta: 6:52:47  iter: 1839  total_loss: 0.7289  loss_cls: 0.3662  loss_box_reg: 0.2684  loss_rpn_cls: 0.02397  loss_rpn_loc: 0.03845  time: 1.8838  data_time: 0.2016  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:44:02 d2.utils.events]: \u001b[0m eta: 6:52:09  iter: 1859  total_loss: 0.6454  loss_cls: 0.3575  loss_box_reg: 0.2542  loss_rpn_cls: 0.02091  loss_rpn_loc: 0.03065  time: 1.8839  data_time: 0.2013  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:44:40 d2.utils.events]: \u001b[0m eta: 6:51:34  iter: 1879  total_loss: 0.7023  loss_cls: 0.363  loss_box_reg: 0.275  loss_rpn_cls: 0.02187  loss_rpn_loc: 0.03303  time: 1.8839  data_time: 0.2029  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:45:18 d2.utils.events]: \u001b[0m eta: 6:50:56  iter: 1899  total_loss: 0.6165  loss_cls: 0.3411  loss_box_reg: 0.2288  loss_rpn_cls: 0.01516  loss_rpn_loc: 0.023  time: 1.8839  data_time: 0.1981  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:45:56 d2.utils.events]: \u001b[0m eta: 6:50:20  iter: 1919  total_loss: 0.7002  loss_cls: 0.3777  loss_box_reg: 0.2642  loss_rpn_cls: 0.02196  loss_rpn_loc: 0.03238  time: 1.8840  data_time: 0.2095  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:46:33 d2.utils.events]: \u001b[0m eta: 6:49:45  iter: 1939  total_loss: 0.7022  loss_cls: 0.361  loss_box_reg: 0.2679  loss_rpn_cls: 0.02498  loss_rpn_loc: 0.03928  time: 1.8840  data_time: 0.2013  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:47:11 d2.utils.events]: \u001b[0m eta: 6:49:09  iter: 1959  total_loss: 0.6477  loss_cls: 0.3429  loss_box_reg: 0.2492  loss_rpn_cls: 0.02206  loss_rpn_loc: 0.0333  time: 1.8841  data_time: 0.2073  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:47:49 d2.utils.events]: \u001b[0m eta: 6:48:28  iter: 1979  total_loss: 0.6434  loss_cls: 0.3455  loss_box_reg: 0.2534  loss_rpn_cls: 0.01974  loss_rpn_loc: 0.02565  time: 1.8840  data_time: 0.1940  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:48:26 d2.utils.events]: \u001b[0m eta: 6:47:49  iter: 1999  total_loss: 0.6607  loss_cls: 0.3549  loss_box_reg: 0.2658  loss_rpn_cls: 0.01724  loss_rpn_loc: 0.02125  time: 1.8840  data_time: 0.1994  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:49:04 d2.utils.events]: \u001b[0m eta: 6:47:10  iter: 2019  total_loss: 0.648  loss_cls: 0.3504  loss_box_reg: 0.2402  loss_rpn_cls: 0.02004  loss_rpn_loc: 0.03956  time: 1.8840  data_time: 0.1979  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:49:42 d2.utils.events]: \u001b[0m eta: 6:46:32  iter: 2039  total_loss: 0.6568  loss_cls: 0.3306  loss_box_reg: 0.2627  loss_rpn_cls: 0.01742  loss_rpn_loc: 0.02445  time: 1.8840  data_time: 0.2004  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:50:20 d2.utils.events]: \u001b[0m eta: 6:45:53  iter: 2059  total_loss: 0.6318  loss_cls: 0.3369  loss_box_reg: 0.265  loss_rpn_cls: 0.02283  loss_rpn_loc: 0.02692  time: 1.8840  data_time: 0.1961  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:50:57 d2.utils.events]: \u001b[0m eta: 6:45:16  iter: 2079  total_loss: 0.7041  loss_cls: 0.3557  loss_box_reg: 0.2672  loss_rpn_cls: 0.01796  loss_rpn_loc: 0.03636  time: 1.8840  data_time: 0.2028  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:51:35 d2.utils.events]: \u001b[0m eta: 6:44:42  iter: 2099  total_loss: 0.6667  loss_cls: 0.3421  loss_box_reg: 0.2447  loss_rpn_cls: 0.02049  loss_rpn_loc: 0.03079  time: 1.8840  data_time: 0.2015  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:52:13 d2.utils.events]: \u001b[0m eta: 6:44:05  iter: 2119  total_loss: 0.6941  loss_cls: 0.3642  loss_box_reg: 0.2699  loss_rpn_cls: 0.02662  loss_rpn_loc: 0.03693  time: 1.8840  data_time: 0.2014  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:52:51 d2.utils.events]: \u001b[0m eta: 6:43:30  iter: 2139  total_loss: 0.6537  loss_cls: 0.3772  loss_box_reg: 0.2455  loss_rpn_cls: 0.02019  loss_rpn_loc: 0.03204  time: 1.8841  data_time: 0.2106  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:53:28 d2.utils.events]: \u001b[0m eta: 6:42:51  iter: 2159  total_loss: 0.6375  loss_cls: 0.3376  loss_box_reg: 0.2433  loss_rpn_cls: 0.02054  loss_rpn_loc: 0.03611  time: 1.8841  data_time: 0.1995  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:54:06 d2.utils.events]: \u001b[0m eta: 6:42:14  iter: 2179  total_loss: 0.6044  loss_cls: 0.3185  loss_box_reg: 0.2258  loss_rpn_cls: 0.02107  loss_rpn_loc: 0.02878  time: 1.8841  data_time: 0.2052  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:54:44 d2.utils.events]: \u001b[0m eta: 6:41:40  iter: 2199  total_loss: 0.6216  loss_cls: 0.3327  loss_box_reg: 0.2495  loss_rpn_cls: 0.0176  loss_rpn_loc: 0.02734  time: 1.8842  data_time: 0.2024  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:55:22 d2.utils.events]: \u001b[0m eta: 6:41:03  iter: 2219  total_loss: 0.5985  loss_cls: 0.3182  loss_box_reg: 0.2269  loss_rpn_cls: 0.01526  loss_rpn_loc: 0.02964  time: 1.8842  data_time: 0.2047  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:55:59 d2.utils.events]: \u001b[0m eta: 6:40:25  iter: 2239  total_loss: 0.5832  loss_cls: 0.3174  loss_box_reg: 0.2495  loss_rpn_cls: 0.0185  loss_rpn_loc: 0.03238  time: 1.8843  data_time: 0.2042  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:56:37 d2.utils.events]: \u001b[0m eta: 6:39:49  iter: 2259  total_loss: 0.6059  loss_cls: 0.3333  loss_box_reg: 0.2448  loss_rpn_cls: 0.01892  loss_rpn_loc: 0.03296  time: 1.8843  data_time: 0.2044  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:57:15 d2.utils.events]: \u001b[0m eta: 6:39:10  iter: 2279  total_loss: 0.6844  loss_cls: 0.3528  loss_box_reg: 0.2594  loss_rpn_cls: 0.02118  loss_rpn_loc: 0.04018  time: 1.8843  data_time: 0.2014  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:57:53 d2.utils.events]: \u001b[0m eta: 6:38:31  iter: 2299  total_loss: 0.6375  loss_cls: 0.3195  loss_box_reg: 0.2396  loss_rpn_cls: 0.01913  loss_rpn_loc: 0.0265  time: 1.8843  data_time: 0.2018  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:58:31 d2.utils.events]: \u001b[0m eta: 6:37:56  iter: 2319  total_loss: 0.6327  loss_cls: 0.3161  loss_box_reg: 0.2424  loss_rpn_cls: 0.01676  loss_rpn_loc: 0.03178  time: 1.8844  data_time: 0.2042  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:59:08 d2.utils.events]: \u001b[0m eta: 6:37:18  iter: 2339  total_loss: 0.6021  loss_cls: 0.3171  loss_box_reg: 0.2372  loss_rpn_cls: 0.01408  loss_rpn_loc: 0.02028  time: 1.8844  data_time: 0.2053  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 16:59:46 d2.utils.events]: \u001b[0m eta: 6:36:40  iter: 2359  total_loss: 0.6576  loss_cls: 0.3514  loss_box_reg: 0.2542  loss_rpn_cls: 0.02092  loss_rpn_loc: 0.03535  time: 1.8844  data_time: 0.1976  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:00:24 d2.utils.events]: \u001b[0m eta: 6:36:06  iter: 2379  total_loss: 0.6464  loss_cls: 0.3392  loss_box_reg: 0.256  loss_rpn_cls: 0.01946  loss_rpn_loc: 0.02953  time: 1.8844  data_time: 0.2037  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:01:02 d2.utils.events]: \u001b[0m eta: 6:35:30  iter: 2399  total_loss: 0.6546  loss_cls: 0.3525  loss_box_reg: 0.2384  loss_rpn_cls: 0.02843  loss_rpn_loc: 0.03066  time: 1.8845  data_time: 0.2036  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:01:39 d2.utils.events]: \u001b[0m eta: 6:34:56  iter: 2419  total_loss: 0.6978  loss_cls: 0.3765  loss_box_reg: 0.2685  loss_rpn_cls: 0.02467  loss_rpn_loc: 0.04296  time: 1.8845  data_time: 0.2026  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:02:17 d2.utils.events]: \u001b[0m eta: 6:34:19  iter: 2439  total_loss: 0.6511  loss_cls: 0.3473  loss_box_reg: 0.2479  loss_rpn_cls: 0.01999  loss_rpn_loc: 0.0312  time: 1.8845  data_time: 0.2034  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:02:55 d2.utils.events]: \u001b[0m eta: 6:33:43  iter: 2459  total_loss: 0.6047  loss_cls: 0.3283  loss_box_reg: 0.2373  loss_rpn_cls: 0.02119  loss_rpn_loc: 0.02499  time: 1.8845  data_time: 0.2037  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:03:33 d2.utils.events]: \u001b[0m eta: 6:33:06  iter: 2479  total_loss: 0.5893  loss_cls: 0.305  loss_box_reg: 0.2446  loss_rpn_cls: 0.02059  loss_rpn_loc: 0.03692  time: 1.8846  data_time: 0.2073  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:04:11 d2.utils.events]: \u001b[0m eta: 6:32:31  iter: 2499  total_loss: 0.6421  loss_cls: 0.3317  loss_box_reg: 0.2736  loss_rpn_cls: 0.01982  loss_rpn_loc: 0.04218  time: 1.8846  data_time: 0.2039  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:04:48 d2.utils.events]: \u001b[0m eta: 6:31:53  iter: 2519  total_loss: 0.5681  loss_cls: 0.3045  loss_box_reg: 0.2375  loss_rpn_cls: 0.01558  loss_rpn_loc: 0.02834  time: 1.8846  data_time: 0.1985  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:05:26 d2.utils.events]: \u001b[0m eta: 6:31:14  iter: 2539  total_loss: 0.6292  loss_cls: 0.3254  loss_box_reg: 0.2514  loss_rpn_cls: 0.022  loss_rpn_loc: 0.02658  time: 1.8847  data_time: 0.2018  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:06:04 d2.utils.events]: \u001b[0m eta: 6:30:33  iter: 2559  total_loss: 0.6355  loss_cls: 0.3198  loss_box_reg: 0.2422  loss_rpn_cls: 0.0175  loss_rpn_loc: 0.03324  time: 1.8847  data_time: 0.2011  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:06:41 d2.utils.events]: \u001b[0m eta: 6:29:57  iter: 2579  total_loss: 0.6316  loss_cls: 0.3276  loss_box_reg: 0.2478  loss_rpn_cls: 0.01785  loss_rpn_loc: 0.03414  time: 1.8847  data_time: 0.2008  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:07:19 d2.utils.events]: \u001b[0m eta: 6:29:20  iter: 2599  total_loss: 0.6473  loss_cls: 0.3441  loss_box_reg: 0.2535  loss_rpn_cls: 0.01747  loss_rpn_loc: 0.02919  time: 1.8847  data_time: 0.2040  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:07:57 d2.utils.events]: \u001b[0m eta: 6:28:44  iter: 2619  total_loss: 0.6835  loss_cls: 0.3585  loss_box_reg: 0.2535  loss_rpn_cls: 0.02439  loss_rpn_loc: 0.02715  time: 1.8847  data_time: 0.2029  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:08:35 d2.utils.events]: \u001b[0m eta: 6:28:07  iter: 2639  total_loss: 0.6008  loss_cls: 0.3158  loss_box_reg: 0.2343  loss_rpn_cls: 0.01931  loss_rpn_loc: 0.02831  time: 1.8848  data_time: 0.2033  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:09:13 d2.utils.events]: \u001b[0m eta: 6:27:30  iter: 2659  total_loss: 0.564  loss_cls: 0.2904  loss_box_reg: 0.2233  loss_rpn_cls: 0.01938  loss_rpn_loc: 0.03359  time: 1.8848  data_time: 0.2027  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:09:50 d2.utils.events]: \u001b[0m eta: 6:26:51  iter: 2679  total_loss: 0.6212  loss_cls: 0.3223  loss_box_reg: 0.2465  loss_rpn_cls: 0.02107  loss_rpn_loc: 0.02842  time: 1.8848  data_time: 0.2007  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:10:28 d2.utils.events]: \u001b[0m eta: 6:26:13  iter: 2699  total_loss: 0.5705  loss_cls: 0.3111  loss_box_reg: 0.2288  loss_rpn_cls: 0.01712  loss_rpn_loc: 0.02316  time: 1.8847  data_time: 0.2008  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:11:06 d2.utils.events]: \u001b[0m eta: 6:25:36  iter: 2719  total_loss: 0.6925  loss_cls: 0.3458  loss_box_reg: 0.2638  loss_rpn_cls: 0.01684  loss_rpn_loc: 0.02981  time: 1.8848  data_time: 0.2052  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:11:44 d2.utils.events]: \u001b[0m eta: 6:24:58  iter: 2739  total_loss: 0.6224  loss_cls: 0.3243  loss_box_reg: 0.2435  loss_rpn_cls: 0.0187  loss_rpn_loc: 0.02716  time: 1.8848  data_time: 0.2016  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:12:21 d2.utils.events]: \u001b[0m eta: 6:24:21  iter: 2759  total_loss: 0.5567  loss_cls: 0.2864  loss_box_reg: 0.2177  loss_rpn_cls: 0.01686  loss_rpn_loc: 0.02776  time: 1.8849  data_time: 0.2060  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:12:59 d2.utils.events]: \u001b[0m eta: 6:23:47  iter: 2779  total_loss: 0.6197  loss_cls: 0.3396  loss_box_reg: 0.2599  loss_rpn_cls: 0.01704  loss_rpn_loc: 0.02908  time: 1.8849  data_time: 0.2045  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:13:37 d2.utils.events]: \u001b[0m eta: 6:23:09  iter: 2799  total_loss: 0.5597  loss_cls: 0.3008  loss_box_reg: 0.2239  loss_rpn_cls: 0.01509  loss_rpn_loc: 0.02708  time: 1.8849  data_time: 0.2078  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:14:15 d2.utils.events]: \u001b[0m eta: 6:22:31  iter: 2819  total_loss: 0.6672  loss_cls: 0.3628  loss_box_reg: 0.2555  loss_rpn_cls: 0.02079  loss_rpn_loc: 0.02909  time: 1.8849  data_time: 0.2050  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:14:52 d2.utils.events]: \u001b[0m eta: 6:21:53  iter: 2839  total_loss: 0.6368  loss_cls: 0.3292  loss_box_reg: 0.2526  loss_rpn_cls: 0.01762  loss_rpn_loc: 0.02945  time: 1.8849  data_time: 0.1984  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:15:30 d2.utils.events]: \u001b[0m eta: 6:21:14  iter: 2859  total_loss: 0.6078  loss_cls: 0.3383  loss_box_reg: 0.2413  loss_rpn_cls: 0.01703  loss_rpn_loc: 0.04151  time: 1.8849  data_time: 0.2005  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:16:08 d2.utils.events]: \u001b[0m eta: 6:20:37  iter: 2879  total_loss: 0.6764  loss_cls: 0.3455  loss_box_reg: 0.2579  loss_rpn_cls: 0.02278  loss_rpn_loc: 0.03813  time: 1.8850  data_time: 0.2070  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:16:46 d2.utils.events]: \u001b[0m eta: 6:19:57  iter: 2899  total_loss: 0.6011  loss_cls: 0.2947  loss_box_reg: 0.2397  loss_rpn_cls: 0.01823  loss_rpn_loc: 0.03002  time: 1.8849  data_time: 0.1985  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:17:23 d2.utils.events]: \u001b[0m eta: 6:19:19  iter: 2919  total_loss: 0.5072  loss_cls: 0.2876  loss_box_reg: 0.2116  loss_rpn_cls: 0.01635  loss_rpn_loc: 0.02803  time: 1.8850  data_time: 0.2010  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:18:01 d2.utils.events]: \u001b[0m eta: 6:18:41  iter: 2939  total_loss: 0.597  loss_cls: 0.309  loss_box_reg: 0.2196  loss_rpn_cls: 0.02115  loss_rpn_loc: 0.02516  time: 1.8850  data_time: 0.2010  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:18:39 d2.utils.events]: \u001b[0m eta: 6:18:03  iter: 2959  total_loss: 0.6006  loss_cls: 0.3257  loss_box_reg: 0.2305  loss_rpn_cls: 0.01699  loss_rpn_loc: 0.03043  time: 1.8850  data_time: 0.2033  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:19:17 d2.utils.events]: \u001b[0m eta: 6:17:31  iter: 2979  total_loss: 0.5809  loss_cls: 0.2966  loss_box_reg: 0.2438  loss_rpn_cls: 0.01535  loss_rpn_loc: 0.02584  time: 1.8850  data_time: 0.2044  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:19:56 d2.data.datasets.coco]: \u001b[0mLoaded 4871 images in COCO format from ../../../dataset/test.json\n",
      "\u001b[32m[01/08 17:19:56 d2.data.build]: \u001b[0mDistribution of instances among all 10 categories:\n",
      "\u001b[36m|   category    | #instances   |  category   | #instances   |  category  | #instances   |\n",
      "|:-------------:|:-------------|:-----------:|:-------------|:----------:|:-------------|\n",
      "| General trash | 0            |    Paper    | 0            | Paper pack | 0            |\n",
      "|     Metal     | 0            |    Glass    | 0            |  Plastic   | 0            |\n",
      "|   Styrofoam   | 0            | Plastic bag | 0            |  Battery   | 0            |\n",
      "|   Clothing    | 0            |             |              |            |              |\n",
      "|     total     | 0            |             |              |            |              |\u001b[0m\n",
      "\u001b[32m[01/08 17:19:56 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[01/08 17:19:56 d2.data.common]: \u001b[0mSerializing 4871 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[01/08 17:19:56 d2.data.common]: \u001b[0mSerialized dataset takes 0.53 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/08 17:19:56 d2.evaluation.coco_evaluation]: \u001b[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001b[32m[01/08 17:19:56 d2.evaluation.evaluator]: \u001b[0mStart inference on 4871 batches\n",
      "\u001b[32m[01/08 17:19:57 d2.evaluation.evaluator]: \u001b[0mInference done 11/4871. Dataloading: 0.0015 s/iter. Inference: 0.0468 s/iter. Eval: 0.0003 s/iter. Total: 0.0485 s/iter. ETA=0:03:55\n",
      "\u001b[32m[01/08 17:20:02 d2.evaluation.evaluator]: \u001b[0mInference done 115/4871. Dataloading: 0.0016 s/iter. Inference: 0.0465 s/iter. Eval: 0.0003 s/iter. Total: 0.0485 s/iter. ETA=0:03:50\n",
      "\u001b[32m[01/08 17:20:07 d2.evaluation.evaluator]: \u001b[0mInference done 226/4871. Dataloading: 0.0016 s/iter. Inference: 0.0450 s/iter. Eval: 0.0003 s/iter. Total: 0.0469 s/iter. ETA=0:03:38\n",
      "\u001b[32m[01/08 17:20:12 d2.evaluation.evaluator]: \u001b[0mInference done 331/4871. Dataloading: 0.0016 s/iter. Inference: 0.0454 s/iter. Eval: 0.0003 s/iter. Total: 0.0473 s/iter. ETA=0:03:34\n",
      "\u001b[32m[01/08 17:20:17 d2.evaluation.evaluator]: \u001b[0mInference done 440/4871. Dataloading: 0.0016 s/iter. Inference: 0.0451 s/iter. Eval: 0.0003 s/iter. Total: 0.0470 s/iter. ETA=0:03:28\n",
      "\u001b[32m[01/08 17:20:22 d2.evaluation.evaluator]: \u001b[0mInference done 546/4871. Dataloading: 0.0016 s/iter. Inference: 0.0452 s/iter. Eval: 0.0003 s/iter. Total: 0.0471 s/iter. ETA=0:03:23\n",
      "\u001b[32m[01/08 17:20:27 d2.evaluation.evaluator]: \u001b[0mInference done 652/4871. Dataloading: 0.0016 s/iter. Inference: 0.0452 s/iter. Eval: 0.0003 s/iter. Total: 0.0471 s/iter. ETA=0:03:18\n",
      "\u001b[32m[01/08 17:20:32 d2.evaluation.evaluator]: \u001b[0mInference done 754/4871. Dataloading: 0.0016 s/iter. Inference: 0.0455 s/iter. Eval: 0.0003 s/iter. Total: 0.0474 s/iter. ETA=0:03:15\n",
      "\u001b[32m[01/08 17:20:37 d2.evaluation.evaluator]: \u001b[0mInference done 863/4871. Dataloading: 0.0016 s/iter. Inference: 0.0453 s/iter. Eval: 0.0003 s/iter. Total: 0.0472 s/iter. ETA=0:03:09\n",
      "\u001b[32m[01/08 17:20:42 d2.evaluation.evaluator]: \u001b[0mInference done 971/4871. Dataloading: 0.0016 s/iter. Inference: 0.0453 s/iter. Eval: 0.0003 s/iter. Total: 0.0472 s/iter. ETA=0:03:04\n",
      "\u001b[32m[01/08 17:20:47 d2.evaluation.evaluator]: \u001b[0mInference done 1076/4871. Dataloading: 0.0016 s/iter. Inference: 0.0453 s/iter. Eval: 0.0003 s/iter. Total: 0.0473 s/iter. ETA=0:02:59\n",
      "\u001b[32m[01/08 17:20:52 d2.evaluation.evaluator]: \u001b[0mInference done 1181/4871. Dataloading: 0.0016 s/iter. Inference: 0.0454 s/iter. Eval: 0.0003 s/iter. Total: 0.0473 s/iter. ETA=0:02:54\n",
      "\u001b[32m[01/08 17:20:57 d2.evaluation.evaluator]: \u001b[0mInference done 1287/4871. Dataloading: 0.0017 s/iter. Inference: 0.0454 s/iter. Eval: 0.0003 s/iter. Total: 0.0473 s/iter. ETA=0:02:49\n",
      "\u001b[32m[01/08 17:21:02 d2.evaluation.evaluator]: \u001b[0mInference done 1395/4871. Dataloading: 0.0017 s/iter. Inference: 0.0453 s/iter. Eval: 0.0003 s/iter. Total: 0.0473 s/iter. ETA=0:02:44\n",
      "\u001b[32m[01/08 17:21:07 d2.evaluation.evaluator]: \u001b[0mInference done 1503/4871. Dataloading: 0.0017 s/iter. Inference: 0.0452 s/iter. Eval: 0.0003 s/iter. Total: 0.0472 s/iter. ETA=0:02:39\n",
      "\u001b[32m[01/08 17:21:13 d2.evaluation.evaluator]: \u001b[0mInference done 1612/4871. Dataloading: 0.0017 s/iter. Inference: 0.0452 s/iter. Eval: 0.0002 s/iter. Total: 0.0472 s/iter. ETA=0:02:33\n",
      "\u001b[32m[01/08 17:21:18 d2.evaluation.evaluator]: \u001b[0mInference done 1719/4871. Dataloading: 0.0017 s/iter. Inference: 0.0451 s/iter. Eval: 0.0003 s/iter. Total: 0.0471 s/iter. ETA=0:02:28\n",
      "\u001b[32m[01/08 17:21:23 d2.evaluation.evaluator]: \u001b[0mInference done 1825/4871. Dataloading: 0.0017 s/iter. Inference: 0.0451 s/iter. Eval: 0.0003 s/iter. Total: 0.0472 s/iter. ETA=0:02:23\n",
      "\u001b[32m[01/08 17:21:28 d2.evaluation.evaluator]: \u001b[0mInference done 1935/4871. Dataloading: 0.0017 s/iter. Inference: 0.0451 s/iter. Eval: 0.0002 s/iter. Total: 0.0471 s/iter. ETA=0:02:18\n",
      "\u001b[32m[01/08 17:21:33 d2.evaluation.evaluator]: \u001b[0mInference done 2044/4871. Dataloading: 0.0017 s/iter. Inference: 0.0450 s/iter. Eval: 0.0002 s/iter. Total: 0.0470 s/iter. ETA=0:02:12\n",
      "\u001b[32m[01/08 17:21:38 d2.evaluation.evaluator]: \u001b[0mInference done 2151/4871. Dataloading: 0.0017 s/iter. Inference: 0.0450 s/iter. Eval: 0.0002 s/iter. Total: 0.0470 s/iter. ETA=0:02:07\n",
      "\u001b[32m[01/08 17:21:43 d2.evaluation.evaluator]: \u001b[0mInference done 2257/4871. Dataloading: 0.0017 s/iter. Inference: 0.0450 s/iter. Eval: 0.0002 s/iter. Total: 0.0471 s/iter. ETA=0:02:02\n",
      "\u001b[32m[01/08 17:21:48 d2.evaluation.evaluator]: \u001b[0mInference done 2367/4871. Dataloading: 0.0017 s/iter. Inference: 0.0450 s/iter. Eval: 0.0002 s/iter. Total: 0.0470 s/iter. ETA=0:01:57\n",
      "\u001b[32m[01/08 17:21:53 d2.evaluation.evaluator]: \u001b[0mInference done 2474/4871. Dataloading: 0.0017 s/iter. Inference: 0.0450 s/iter. Eval: 0.0002 s/iter. Total: 0.0470 s/iter. ETA=0:01:52\n",
      "\u001b[32m[01/08 17:21:58 d2.evaluation.evaluator]: \u001b[0mInference done 2580/4871. Dataloading: 0.0017 s/iter. Inference: 0.0450 s/iter. Eval: 0.0002 s/iter. Total: 0.0470 s/iter. ETA=0:01:47\n",
      "\u001b[32m[01/08 17:22:03 d2.evaluation.evaluator]: \u001b[0mInference done 2686/4871. Dataloading: 0.0017 s/iter. Inference: 0.0450 s/iter. Eval: 0.0002 s/iter. Total: 0.0470 s/iter. ETA=0:01:42\n",
      "\u001b[32m[01/08 17:22:08 d2.evaluation.evaluator]: \u001b[0mInference done 2790/4871. Dataloading: 0.0017 s/iter. Inference: 0.0450 s/iter. Eval: 0.0002 s/iter. Total: 0.0471 s/iter. ETA=0:01:37\n",
      "\u001b[32m[01/08 17:22:13 d2.evaluation.evaluator]: \u001b[0mInference done 2898/4871. Dataloading: 0.0017 s/iter. Inference: 0.0450 s/iter. Eval: 0.0002 s/iter. Total: 0.0470 s/iter. ETA=0:01:32\n",
      "\u001b[32m[01/08 17:22:18 d2.evaluation.evaluator]: \u001b[0mInference done 3007/4871. Dataloading: 0.0017 s/iter. Inference: 0.0450 s/iter. Eval: 0.0002 s/iter. Total: 0.0470 s/iter. ETA=0:01:27\n",
      "\u001b[32m[01/08 17:22:23 d2.evaluation.evaluator]: \u001b[0mInference done 3114/4871. Dataloading: 0.0017 s/iter. Inference: 0.0450 s/iter. Eval: 0.0002 s/iter. Total: 0.0470 s/iter. ETA=0:01:22\n",
      "\u001b[32m[01/08 17:22:28 d2.evaluation.evaluator]: \u001b[0mInference done 3221/4871. Dataloading: 0.0017 s/iter. Inference: 0.0450 s/iter. Eval: 0.0002 s/iter. Total: 0.0470 s/iter. ETA=0:01:17\n",
      "\u001b[32m[01/08 17:22:33 d2.evaluation.evaluator]: \u001b[0mInference done 3329/4871. Dataloading: 0.0017 s/iter. Inference: 0.0450 s/iter. Eval: 0.0002 s/iter. Total: 0.0470 s/iter. ETA=0:01:12\n",
      "\u001b[32m[01/08 17:22:38 d2.evaluation.evaluator]: \u001b[0mInference done 3437/4871. Dataloading: 0.0017 s/iter. Inference: 0.0449 s/iter. Eval: 0.0002 s/iter. Total: 0.0470 s/iter. ETA=0:01:07\n",
      "\u001b[32m[01/08 17:22:43 d2.evaluation.evaluator]: \u001b[0mInference done 3543/4871. Dataloading: 0.0017 s/iter. Inference: 0.0450 s/iter. Eval: 0.0002 s/iter. Total: 0.0470 s/iter. ETA=0:01:02\n",
      "\u001b[32m[01/08 17:22:48 d2.evaluation.evaluator]: \u001b[0mInference done 3650/4871. Dataloading: 0.0017 s/iter. Inference: 0.0450 s/iter. Eval: 0.0002 s/iter. Total: 0.0470 s/iter. ETA=0:00:57\n",
      "\u001b[32m[01/08 17:22:53 d2.evaluation.evaluator]: \u001b[0mInference done 3760/4871. Dataloading: 0.0017 s/iter. Inference: 0.0449 s/iter. Eval: 0.0002 s/iter. Total: 0.0470 s/iter. ETA=0:00:52\n",
      "\u001b[32m[01/08 17:22:58 d2.evaluation.evaluator]: \u001b[0mInference done 3868/4871. Dataloading: 0.0017 s/iter. Inference: 0.0449 s/iter. Eval: 0.0002 s/iter. Total: 0.0469 s/iter. ETA=0:00:47\n",
      "\u001b[32m[01/08 17:23:03 d2.evaluation.evaluator]: \u001b[0mInference done 3974/4871. Dataloading: 0.0017 s/iter. Inference: 0.0449 s/iter. Eval: 0.0002 s/iter. Total: 0.0469 s/iter. ETA=0:00:42\n",
      "\u001b[32m[01/08 17:23:08 d2.evaluation.evaluator]: \u001b[0mInference done 4083/4871. Dataloading: 0.0017 s/iter. Inference: 0.0449 s/iter. Eval: 0.0002 s/iter. Total: 0.0469 s/iter. ETA=0:00:36\n",
      "\u001b[32m[01/08 17:23:13 d2.evaluation.evaluator]: \u001b[0mInference done 4185/4871. Dataloading: 0.0017 s/iter. Inference: 0.0449 s/iter. Eval: 0.0002 s/iter. Total: 0.0470 s/iter. ETA=0:00:32\n",
      "\u001b[32m[01/08 17:23:18 d2.evaluation.evaluator]: \u001b[0mInference done 4294/4871. Dataloading: 0.0017 s/iter. Inference: 0.0449 s/iter. Eval: 0.0002 s/iter. Total: 0.0470 s/iter. ETA=0:00:27\n",
      "\u001b[32m[01/08 17:23:23 d2.evaluation.evaluator]: \u001b[0mInference done 4402/4871. Dataloading: 0.0017 s/iter. Inference: 0.0449 s/iter. Eval: 0.0002 s/iter. Total: 0.0469 s/iter. ETA=0:00:22\n",
      "\u001b[32m[01/08 17:23:28 d2.evaluation.evaluator]: \u001b[0mInference done 4501/4871. Dataloading: 0.0017 s/iter. Inference: 0.0450 s/iter. Eval: 0.0002 s/iter. Total: 0.0470 s/iter. ETA=0:00:17\n",
      "\u001b[32m[01/08 17:23:33 d2.evaluation.evaluator]: \u001b[0mInference done 4607/4871. Dataloading: 0.0017 s/iter. Inference: 0.0450 s/iter. Eval: 0.0002 s/iter. Total: 0.0470 s/iter. ETA=0:00:12\n",
      "\u001b[32m[01/08 17:23:38 d2.evaluation.evaluator]: \u001b[0mInference done 4712/4871. Dataloading: 0.0017 s/iter. Inference: 0.0450 s/iter. Eval: 0.0002 s/iter. Total: 0.0471 s/iter. ETA=0:00:07\n",
      "\u001b[32m[01/08 17:23:43 d2.evaluation.evaluator]: \u001b[0mInference done 4820/4871. Dataloading: 0.0017 s/iter. Inference: 0.0450 s/iter. Eval: 0.0002 s/iter. Total: 0.0470 s/iter. ETA=0:00:02\n",
      "\u001b[32m[01/08 17:23:46 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:03:48.922799 (0.047045 s / iter per device, on 1 devices)\n",
      "\u001b[32m[01/08 17:23:46 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:03:38 (0.045006 s / iter per device, on 1 devices)\n",
      "\u001b[32m[01/08 17:23:46 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[01/08 17:23:47 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output_eval/coco_instances_results.json\n",
      "\u001b[32m[01/08 17:23:47 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.66s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[01/08 17:23:48 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[01/08 17:23:50 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 1.98 seconds.\n",
      "\u001b[32m[01/08 17:23:50 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[01/08 17:23:50 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.25 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "\u001b[32m[01/08 17:23:50 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|  AP  |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| nan  |  nan   |  nan   |  nan  |  nan  |  nan  |\n",
      "\u001b[32m[01/08 17:23:50 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001b[32m[01/08 17:23:50 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category      | AP   | category    | AP   | category   | AP   |\n",
      "|:--------------|:-----|:------------|:-----|:-----------|:-----|\n",
      "| General trash | nan  | Paper       | nan  | Paper pack | nan  |\n",
      "| Metal         | nan  | Glass       | nan  | Plastic    | nan  |\n",
      "| Styrofoam     | nan  | Plastic bag | nan  | Battery    | nan  |\n",
      "| Clothing      | nan  |             |      |            |      |\n",
      "\u001b[32m[01/08 17:23:50 d2.engine.defaults]: \u001b[0mEvaluation results for coco_trash_test in csv format:\n",
      "\u001b[32m[01/08 17:23:50 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
      "\u001b[32m[01/08 17:23:50 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
      "\u001b[32m[01/08 17:23:50 d2.evaluation.testing]: \u001b[0mcopypaste: nan,nan,nan,nan,nan,nan\n",
      "\u001b[32m[01/08 17:23:50 d2.utils.events]: \u001b[0m eta: 6:16:54  iter: 2999  total_loss: 0.5478  loss_cls: 0.3045  loss_box_reg: 0.2017  loss_rpn_cls: 0.01441  loss_rpn_loc: 0.02134  time: 1.8851  data_time: 0.2061  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:24:29 d2.utils.events]: \u001b[0m eta: 6:16:19  iter: 3019  total_loss: 0.6368  loss_cls: 0.329  loss_box_reg: 0.2441  loss_rpn_cls: 0.02188  loss_rpn_loc: 0.03359  time: 1.8852  data_time: 0.2105  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:25:06 d2.utils.events]: \u001b[0m eta: 6:15:41  iter: 3039  total_loss: 0.66  loss_cls: 0.3346  loss_box_reg: 0.2587  loss_rpn_cls: 0.01967  loss_rpn_loc: 0.03399  time: 1.8851  data_time: 0.1999  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:25:44 d2.utils.events]: \u001b[0m eta: 6:15:06  iter: 3059  total_loss: 0.5801  loss_cls: 0.2993  loss_box_reg: 0.2405  loss_rpn_cls: 0.0203  loss_rpn_loc: 0.03608  time: 1.8852  data_time: 0.2115  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:26:22 d2.utils.events]: \u001b[0m eta: 6:14:29  iter: 3079  total_loss: 0.5493  loss_cls: 0.2901  loss_box_reg: 0.221  loss_rpn_cls: 0.01986  loss_rpn_loc: 0.02604  time: 1.8853  data_time: 0.2047  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:27:00 d2.utils.events]: \u001b[0m eta: 6:13:51  iter: 3099  total_loss: 0.6344  loss_cls: 0.3219  loss_box_reg: 0.2408  loss_rpn_cls: 0.01686  loss_rpn_loc: 0.02857  time: 1.8853  data_time: 0.2034  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:27:38 d2.utils.events]: \u001b[0m eta: 6:13:14  iter: 3119  total_loss: 0.5785  loss_cls: 0.3054  loss_box_reg: 0.2333  loss_rpn_cls: 0.02111  loss_rpn_loc: 0.03883  time: 1.8853  data_time: 0.2104  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:28:15 d2.utils.events]: \u001b[0m eta: 6:12:35  iter: 3139  total_loss: 0.6062  loss_cls: 0.2972  loss_box_reg: 0.2316  loss_rpn_cls: 0.01933  loss_rpn_loc: 0.04135  time: 1.8853  data_time: 0.2003  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:28:53 d2.utils.events]: \u001b[0m eta: 6:11:55  iter: 3159  total_loss: 0.5703  loss_cls: 0.3125  loss_box_reg: 0.2047  loss_rpn_cls: 0.01374  loss_rpn_loc: 0.02206  time: 1.8853  data_time: 0.1964  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:29:31 d2.utils.events]: \u001b[0m eta: 6:11:17  iter: 3179  total_loss: 0.6078  loss_cls: 0.3285  loss_box_reg: 0.2397  loss_rpn_cls: 0.01761  loss_rpn_loc: 0.02815  time: 1.8853  data_time: 0.2031  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:30:08 d2.utils.events]: \u001b[0m eta: 6:10:38  iter: 3199  total_loss: 0.5743  loss_cls: 0.3126  loss_box_reg: 0.2161  loss_rpn_cls: 0.01931  loss_rpn_loc: 0.03658  time: 1.8853  data_time: 0.2019  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:30:46 d2.utils.events]: \u001b[0m eta: 6:10:02  iter: 3219  total_loss: 0.6203  loss_cls: 0.3036  loss_box_reg: 0.2456  loss_rpn_cls: 0.01903  loss_rpn_loc: 0.03954  time: 1.8853  data_time: 0.1997  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:31:24 d2.utils.events]: \u001b[0m eta: 6:09:25  iter: 3239  total_loss: 0.5711  loss_cls: 0.2962  loss_box_reg: 0.2273  loss_rpn_cls: 0.01887  loss_rpn_loc: 0.0309  time: 1.8853  data_time: 0.2002  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:32:02 d2.utils.events]: \u001b[0m eta: 6:08:47  iter: 3259  total_loss: 0.5776  loss_cls: 0.3022  loss_box_reg: 0.2399  loss_rpn_cls: 0.01714  loss_rpn_loc: 0.02669  time: 1.8853  data_time: 0.2002  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:32:40 d2.utils.events]: \u001b[0m eta: 6:08:09  iter: 3279  total_loss: 0.6441  loss_cls: 0.3251  loss_box_reg: 0.2622  loss_rpn_cls: 0.021  loss_rpn_loc: 0.04205  time: 1.8854  data_time: 0.2047  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:33:17 d2.utils.events]: \u001b[0m eta: 6:07:32  iter: 3299  total_loss: 0.5992  loss_cls: 0.3076  loss_box_reg: 0.2401  loss_rpn_cls: 0.01705  loss_rpn_loc: 0.02839  time: 1.8854  data_time: 0.2011  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:33:55 d2.utils.events]: \u001b[0m eta: 6:06:54  iter: 3319  total_loss: 0.5714  loss_cls: 0.2986  loss_box_reg: 0.2223  loss_rpn_cls: 0.01324  loss_rpn_loc: 0.02206  time: 1.8854  data_time: 0.2009  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:34:33 d2.utils.events]: \u001b[0m eta: 6:06:16  iter: 3339  total_loss: 0.5885  loss_cls: 0.2845  loss_box_reg: 0.2611  loss_rpn_cls: 0.01754  loss_rpn_loc: 0.02474  time: 1.8854  data_time: 0.1995  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:35:10 d2.utils.events]: \u001b[0m eta: 6:05:40  iter: 3359  total_loss: 0.5763  loss_cls: 0.3026  loss_box_reg: 0.2258  loss_rpn_cls: 0.01492  loss_rpn_loc: 0.02739  time: 1.8854  data_time: 0.2015  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:35:48 d2.utils.events]: \u001b[0m eta: 6:05:03  iter: 3379  total_loss: 0.5795  loss_cls: 0.3063  loss_box_reg: 0.2164  loss_rpn_cls: 0.01387  loss_rpn_loc: 0.02871  time: 1.8854  data_time: 0.2053  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:36:26 d2.utils.events]: \u001b[0m eta: 6:04:26  iter: 3399  total_loss: 0.5813  loss_cls: 0.2995  loss_box_reg: 0.2391  loss_rpn_cls: 0.02021  loss_rpn_loc: 0.03207  time: 1.8854  data_time: 0.2064  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:37:04 d2.utils.events]: \u001b[0m eta: 6:03:49  iter: 3419  total_loss: 0.5801  loss_cls: 0.2906  loss_box_reg: 0.2396  loss_rpn_cls: 0.01969  loss_rpn_loc: 0.02766  time: 1.8855  data_time: 0.2067  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:37:42 d2.utils.events]: \u001b[0m eta: 6:03:13  iter: 3439  total_loss: 0.559  loss_cls: 0.2969  loss_box_reg: 0.2248  loss_rpn_cls: 0.01478  loss_rpn_loc: 0.02479  time: 1.8855  data_time: 0.2085  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:38:20 d2.utils.events]: \u001b[0m eta: 6:02:35  iter: 3459  total_loss: 0.5593  loss_cls: 0.2831  loss_box_reg: 0.2225  loss_rpn_cls: 0.01557  loss_rpn_loc: 0.03791  time: 1.8855  data_time: 0.2033  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:38:58 d2.utils.events]: \u001b[0m eta: 6:01:57  iter: 3479  total_loss: 0.5617  loss_cls: 0.2747  loss_box_reg: 0.2268  loss_rpn_cls: 0.017  loss_rpn_loc: 0.03613  time: 1.8856  data_time: 0.2022  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:39:35 d2.utils.events]: \u001b[0m eta: 6:01:21  iter: 3499  total_loss: 0.5897  loss_cls: 0.2938  loss_box_reg: 0.2189  loss_rpn_cls: 0.01633  loss_rpn_loc: 0.02872  time: 1.8856  data_time: 0.2053  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:40:13 d2.utils.events]: \u001b[0m eta: 6:00:45  iter: 3519  total_loss: 0.6412  loss_cls: 0.3407  loss_box_reg: 0.2371  loss_rpn_cls: 0.02368  loss_rpn_loc: 0.03211  time: 1.8856  data_time: 0.2004  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:40:51 d2.utils.events]: \u001b[0m eta: 6:00:08  iter: 3539  total_loss: 0.5856  loss_cls: 0.2835  loss_box_reg: 0.235  loss_rpn_cls: 0.01912  loss_rpn_loc: 0.0356  time: 1.8856  data_time: 0.2034  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:41:29 d2.utils.events]: \u001b[0m eta: 5:59:31  iter: 3559  total_loss: 0.5611  loss_cls: 0.2843  loss_box_reg: 0.2372  loss_rpn_cls: 0.01638  loss_rpn_loc: 0.02386  time: 1.8856  data_time: 0.2073  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:42:06 d2.utils.events]: \u001b[0m eta: 5:58:53  iter: 3579  total_loss: 0.6075  loss_cls: 0.2998  loss_box_reg: 0.2539  loss_rpn_cls: 0.02086  loss_rpn_loc: 0.03565  time: 1.8857  data_time: 0.2046  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:42:44 d2.utils.events]: \u001b[0m eta: 5:58:16  iter: 3599  total_loss: 0.5578  loss_cls: 0.2844  loss_box_reg: 0.2158  loss_rpn_cls: 0.01507  loss_rpn_loc: 0.01987  time: 1.8857  data_time: 0.2060  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:43:22 d2.utils.events]: \u001b[0m eta: 5:57:38  iter: 3619  total_loss: 0.556  loss_cls: 0.2842  loss_box_reg: 0.2081  loss_rpn_cls: 0.01883  loss_rpn_loc: 0.02876  time: 1.8857  data_time: 0.1997  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:44:00 d2.utils.events]: \u001b[0m eta: 5:56:58  iter: 3639  total_loss: 0.58  loss_cls: 0.2926  loss_box_reg: 0.2345  loss_rpn_cls: 0.01507  loss_rpn_loc: 0.02962  time: 1.8857  data_time: 0.2015  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:44:37 d2.utils.events]: \u001b[0m eta: 5:56:21  iter: 3659  total_loss: 0.5913  loss_cls: 0.2953  loss_box_reg: 0.2396  loss_rpn_cls: 0.02005  loss_rpn_loc: 0.03067  time: 1.8857  data_time: 0.2045  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:45:15 d2.utils.events]: \u001b[0m eta: 5:55:45  iter: 3679  total_loss: 0.5556  loss_cls: 0.2788  loss_box_reg: 0.226  loss_rpn_cls: 0.0153  loss_rpn_loc: 0.02288  time: 1.8857  data_time: 0.2038  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:45:53 d2.utils.events]: \u001b[0m eta: 5:55:08  iter: 3699  total_loss: 0.5378  loss_cls: 0.2772  loss_box_reg: 0.2111  loss_rpn_cls: 0.01499  loss_rpn_loc: 0.03248  time: 1.8857  data_time: 0.2009  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:46:31 d2.utils.events]: \u001b[0m eta: 5:54:30  iter: 3719  total_loss: 0.5926  loss_cls: 0.2907  loss_box_reg: 0.2391  loss_rpn_cls: 0.01709  loss_rpn_loc: 0.02962  time: 1.8857  data_time: 0.1997  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:47:09 d2.utils.events]: \u001b[0m eta: 5:53:53  iter: 3739  total_loss: 0.5565  loss_cls: 0.2802  loss_box_reg: 0.2182  loss_rpn_cls: 0.01651  loss_rpn_loc: 0.02726  time: 1.8857  data_time: 0.2079  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:47:46 d2.utils.events]: \u001b[0m eta: 5:53:15  iter: 3759  total_loss: 0.5747  loss_cls: 0.2864  loss_box_reg: 0.2245  loss_rpn_cls: 0.01945  loss_rpn_loc: 0.03719  time: 1.8858  data_time: 0.2017  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:48:24 d2.utils.events]: \u001b[0m eta: 5:52:38  iter: 3779  total_loss: 0.6056  loss_cls: 0.3122  loss_box_reg: 0.2475  loss_rpn_cls: 0.01807  loss_rpn_loc: 0.03546  time: 1.8858  data_time: 0.1991  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:49:02 d2.utils.events]: \u001b[0m eta: 5:52:01  iter: 3799  total_loss: 0.5473  loss_cls: 0.2814  loss_box_reg: 0.2131  loss_rpn_cls: 0.01659  loss_rpn_loc: 0.0299  time: 1.8858  data_time: 0.2055  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:49:40 d2.utils.events]: \u001b[0m eta: 5:51:24  iter: 3819  total_loss: 0.5905  loss_cls: 0.2862  loss_box_reg: 0.2432  loss_rpn_cls: 0.01958  loss_rpn_loc: 0.03599  time: 1.8858  data_time: 0.2013  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:50:17 d2.utils.events]: \u001b[0m eta: 5:50:48  iter: 3839  total_loss: 0.5348  loss_cls: 0.2781  loss_box_reg: 0.2169  loss_rpn_cls: 0.01639  loss_rpn_loc: 0.02629  time: 1.8858  data_time: 0.1981  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:50:55 d2.utils.events]: \u001b[0m eta: 5:50:12  iter: 3859  total_loss: 0.5978  loss_cls: 0.2961  loss_box_reg: 0.2383  loss_rpn_cls: 0.01965  loss_rpn_loc: 0.03376  time: 1.8858  data_time: 0.2085  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:51:33 d2.utils.events]: \u001b[0m eta: 5:49:34  iter: 3879  total_loss: 0.6015  loss_cls: 0.301  loss_box_reg: 0.25  loss_rpn_cls: 0.0211  loss_rpn_loc: 0.03298  time: 1.8859  data_time: 0.2014  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:52:11 d2.utils.events]: \u001b[0m eta: 5:48:59  iter: 3899  total_loss: 0.5443  loss_cls: 0.2999  loss_box_reg: 0.2102  loss_rpn_cls: 0.01497  loss_rpn_loc: 0.02348  time: 1.8859  data_time: 0.2060  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:52:49 d2.utils.events]: \u001b[0m eta: 5:48:21  iter: 3919  total_loss: 0.5979  loss_cls: 0.2978  loss_box_reg: 0.2318  loss_rpn_cls: 0.01587  loss_rpn_loc: 0.03335  time: 1.8859  data_time: 0.2034  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:53:27 d2.utils.events]: \u001b[0m eta: 5:47:44  iter: 3939  total_loss: 0.5689  loss_cls: 0.2835  loss_box_reg: 0.222  loss_rpn_cls: 0.01611  loss_rpn_loc: 0.02356  time: 1.8859  data_time: 0.2034  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:54:05 d2.utils.events]: \u001b[0m eta: 5:47:06  iter: 3959  total_loss: 0.585  loss_cls: 0.3047  loss_box_reg: 0.2271  loss_rpn_cls: 0.01812  loss_rpn_loc: 0.03252  time: 1.8860  data_time: 0.2030  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:54:42 d2.utils.events]: \u001b[0m eta: 5:46:26  iter: 3979  total_loss: 0.507  loss_cls: 0.2701  loss_box_reg: 0.2015  loss_rpn_cls: 0.01137  loss_rpn_loc: 0.0223  time: 1.8860  data_time: 0.2011  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:55:20 d2.utils.events]: \u001b[0m eta: 5:45:47  iter: 3999  total_loss: 0.5498  loss_cls: 0.2744  loss_box_reg: 0.2203  loss_rpn_cls: 0.01779  loss_rpn_loc: 0.03275  time: 1.8860  data_time: 0.2035  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:55:58 d2.utils.events]: \u001b[0m eta: 5:45:07  iter: 4019  total_loss: 0.572  loss_cls: 0.2908  loss_box_reg: 0.2241  loss_rpn_cls: 0.01895  loss_rpn_loc: 0.02954  time: 1.8859  data_time: 0.1987  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:56:35 d2.utils.events]: \u001b[0m eta: 5:44:31  iter: 4039  total_loss: 0.5645  loss_cls: 0.2831  loss_box_reg: 0.2352  loss_rpn_cls: 0.01924  loss_rpn_loc: 0.03333  time: 1.8859  data_time: 0.1991  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:57:13 d2.utils.events]: \u001b[0m eta: 5:43:53  iter: 4059  total_loss: 0.5571  loss_cls: 0.292  loss_box_reg: 0.2332  loss_rpn_cls: 0.01354  loss_rpn_loc: 0.0258  time: 1.8860  data_time: 0.2038  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:57:51 d2.utils.events]: \u001b[0m eta: 5:43:14  iter: 4079  total_loss: 0.5585  loss_cls: 0.2798  loss_box_reg: 0.234  loss_rpn_cls: 0.02165  loss_rpn_loc: 0.02712  time: 1.8860  data_time: 0.2005  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:58:29 d2.utils.events]: \u001b[0m eta: 5:42:35  iter: 4099  total_loss: 0.5686  loss_cls: 0.2957  loss_box_reg: 0.2275  loss_rpn_cls: 0.01588  loss_rpn_loc: 0.03444  time: 1.8859  data_time: 0.2002  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:59:06 d2.utils.events]: \u001b[0m eta: 5:41:56  iter: 4119  total_loss: 0.506  loss_cls: 0.2686  loss_box_reg: 0.2195  loss_rpn_cls: 0.01438  loss_rpn_loc: 0.02549  time: 1.8859  data_time: 0.2012  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 17:59:44 d2.utils.events]: \u001b[0m eta: 5:41:18  iter: 4139  total_loss: 0.6136  loss_cls: 0.2882  loss_box_reg: 0.2444  loss_rpn_cls: 0.02081  loss_rpn_loc: 0.02971  time: 1.8859  data_time: 0.1996  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 18:00:22 d2.utils.events]: \u001b[0m eta: 5:40:43  iter: 4159  total_loss: 0.5369  loss_cls: 0.2799  loss_box_reg: 0.2025  loss_rpn_cls: 0.01466  loss_rpn_loc: 0.02746  time: 1.8860  data_time: 0.2087  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 18:01:00 d2.utils.events]: \u001b[0m eta: 5:40:07  iter: 4179  total_loss: 0.5199  loss_cls: 0.253  loss_box_reg: 0.2175  loss_rpn_cls: 0.01184  loss_rpn_loc: 0.02834  time: 1.8860  data_time: 0.2028  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 18:01:38 d2.utils.events]: \u001b[0m eta: 5:39:29  iter: 4199  total_loss: 0.5452  loss_cls: 0.2767  loss_box_reg: 0.2161  loss_rpn_cls: 0.01938  loss_rpn_loc: 0.02387  time: 1.8860  data_time: 0.2021  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 18:02:15 d2.utils.events]: \u001b[0m eta: 5:38:51  iter: 4219  total_loss: 0.5988  loss_cls: 0.3073  loss_box_reg: 0.2359  loss_rpn_cls: 0.01978  loss_rpn_loc: 0.0274  time: 1.8860  data_time: 0.2028  lr: 0.001  max_mem: 24877M\n",
      "\u001b[32m[01/08 18:02:53 d2.utils.events]: \u001b[0m eta: 5:38:12  iter: 4239  total_loss: 0.5897  loss_cls: 0.2967  loss_box_reg: 0.2383  loss_rpn_cls: 0.01395  loss_rpn_loc: 0.02902  time: 1.8860  data_time: 0.1991  lr: 0.001  max_mem: 24877M\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
