{"cells":[{"cell_type":"markdown","metadata":{"id":"uJoYX_daAzR7"},"source":["# Object Detection - Mission 6\n","#### EfficientDet 추론하기\n","EfficientDet은 보다 더 높은 정확도와 효율성을 가진 detector 구조입니다. 앞서 구현했던 EfficientDet 모델을 사용하여 inference 결과를 만들어주는 코드를 작성해보는 시간을 가져보겠습니다.\n","<br>EfficientDet 모델의 자세한 구조는 06강: EfficientDet 강의를 참고합니다."]},{"cell_type":"markdown","metadata":{"id":"9imdWD6TBT8n"},"source":["## 대회 데이터셋 구성\n","Custom 데이터를 구현하여 대회 데이터셋에 EfficientDet 모델을 추론해봅니다. <br>\n","데이터셋의 자세한 개요는 [대회 플랫폼](https://next.stages.ai/competitions/)의 데이터 설명을 참고합니다.\n","> Copyright: CC BY 2.0\n","\n","### dataset\n","    ├── train.json\n","    ├── test.json\n","    ├── train\n","    └── test"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"GLb2lMFmNl8O"},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["# 라이브러리 및 모듈 import\n","from pycocotools.coco import COCO\n","from pycocotools.cocoeval import COCOeval\n","import numpy as np\n","import cv2\n","import os\n","import torch\n","from torch.utils.data import DataLoader, Dataset\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain\n","from effdet.efficientdet import HeadNet\n","import pandas as pd\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"eVbwnEcyNl8b"},"outputs":[],"source":["# CustomDataset class 선언\n","\n","class CustomDataset(Dataset):\n","    '''\n","      data_dir: data가 존재하는 폴더 경로\n","      transforms: data transform (resize, crop, Totensor, etc,,,)\n","    '''\n","\n","    def __init__(self, annotation, data_dir, transforms=None):\n","        super().__init__()\n","        self.data_dir = data_dir\n","        # coco annotation 불러오기 (coco API)\n","        self.coco = COCO(annotation)\n","        self.predictions = {\n","            \"images\": self.coco.dataset[\"images\"].copy(),\n","            \"categories\": self.coco.dataset[\"categories\"].copy(),\n","            \"annotations\": None\n","        }\n","        self.transforms = transforms\n","\n","    def __getitem__(self, index: int):\n","        image_id = self.coco.getImgIds(imgIds=index)\n","\n","        image_info = self.coco.loadImgs(image_id)[0]\n","\n","        image = cv2.imread(os.path.join(self.data_dir, image_info['file_name']))\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n","        image /= 255.0\n","\n","        # 라벨 등 이미지 외 다른 정보 없기 때문에 train dataset과 달리 이미지만 전처리\n","\n","        # transform\n","        if self.transforms:\n","            sample = self.transforms(image=image)\n","\n","        return sample['image'], image_id\n","\n","    def __len__(self) -> int:\n","        return len(self.coco.getImgIds())"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"8XAO7td8Nl8d"},"outputs":[],"source":["# Albumentation을 이용, augmentation 선언\n","def get_train_transform():\n","    return A.Compose([\n","        A.Resize(512, 512),\n","        A.Flip(p=0.5),\n","        ToTensorV2(p=1.0)\n","    ])\n","\n","\n","def get_valid_transform():\n","    return A.Compose([\n","        A.Resize(512, 512),\n","        ToTensorV2(p=1.0)\n","    ])"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"R_XLukpXNl8e"},"outputs":[],"source":["from effdet import DetBenchPredict\n","import gc\n","\n","# Effdet config를 통해 모델 불러오기 + ckpt load\n","def load_net(checkpoint_path, device):\n","    config = get_efficientdet_config('tf_efficientdet_d3')\n","    config.num_classes = 10\n","    config.image_size = (512,512)\n","\n","    config.soft_nms = False\n","    config.max_det_per_image = 25\n","\n","    net = EfficientDet(config, pretrained_backbone=False)\n","    net.class_net = HeadNet(config, num_outputs=config.num_classes)\n","\n","    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n","\n","    net = DetBenchPredict(net)\n","    net.load_state_dict(checkpoint)\n","    net.eval()\n","\n","    return net.to(device)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"zM1rE2zCNl8g"},"outputs":[],"source":["# valid function\n","def valid_fn(val_data_loader, model, device):\n","    outputs = []\n","    for images, image_ids in tqdm(val_data_loader):\n","        # gpu 계산을 위해 image.to(device)\n","        images = torch.stack(images) # bs, ch, w, h\n","        images = images.to(device).float()\n","        output = model(images)\n","        for out in output:\n","            outputs.append({'boxes': out.detach().cpu().numpy()[:,:4],\n","                            'scores': out.detach().cpu().numpy()[:,4],\n","                            'labels': out.detach().cpu().numpy()[:,-1]})\n","    return outputs\n","\n","def collate_fn(batch):\n","    return tuple(zip(*batch))"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"eH4GRrXlNl8l"},"outputs":[],"source":["def main():\n","    annotation = '../../../dataset/test.json'\n","    data_dir = '../../../dataset'\n","    val_dataset = CustomDataset(annotation, data_dir, get_valid_transform())\n","    epoch = 1\n","    checkpoint_path = f'epoch_{epoch}.pth'\n","    score_threshold = 0.1\n","    val_data_loader = DataLoader(\n","        val_dataset,\n","        batch_size=4,\n","        shuffle=False,\n","        num_workers=4,\n","        collate_fn=collate_fn\n","    )\n","\n","    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","    print(device)\n","\n","    model = load_net(checkpoint_path, device)\n","\n","    outputs = valid_fn(val_data_loader, model, device)\n","\n","    prediction_strings = []\n","    file_names = []\n","    coco = COCO(annotation)\n","\n","    for i, output in enumerate(outputs):\n","        prediction_string = ''\n","        image_info = coco.loadImgs(coco.getImgIds(imgIds=i))[0]\n","        for box, score, label in zip(output['boxes'], output['scores'], output['labels']):\n","            if score > score_threshold:\n","                prediction_string += str(int(label)) + ' ' + str(score) + ' ' + str(box[0]) + ' ' + str(\n","                    box[1]) + ' ' + str(box[2]) + ' ' + str(box[3]) + ' '\n","        prediction_strings.append(prediction_string)\n","        file_names.append(image_info['file_name'])\n","\n","    submission = pd.DataFrame()\n","    submission['PredictionString'] = prediction_strings\n","    submission['image_id'] = file_names\n","    submission.to_csv(f'submission_{epoch}.csv', index=None)\n","    print(submission.head())"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"1TpoSuGCNl8n","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["loading annotations into memory...\n","Done (t=0.01s)\n","creating index...\n","index created!\n","cuda\n"]},{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'epoch_1.pth'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[8], line 19\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(device)\n\u001b[0;32m---> 19\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m outputs \u001b[38;5;241m=\u001b[39m valid_fn(val_data_loader, model, device)\n\u001b[1;32m     23\u001b[0m prediction_strings \u001b[38;5;241m=\u001b[39m []\n","Cell \u001b[0;32mIn[4], line 16\u001b[0m, in \u001b[0;36mload_net\u001b[0;34m(checkpoint_path, device)\u001b[0m\n\u001b[1;32m     13\u001b[0m net \u001b[38;5;241m=\u001b[39m EfficientDet(config, pretrained_backbone\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m net\u001b[38;5;241m.\u001b[39mclass_net \u001b[38;5;241m=\u001b[39m HeadNet(config, num_outputs\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_classes)\n\u001b[0;32m---> 16\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m net \u001b[38;5;241m=\u001b[39m DetBenchPredict(net)\n\u001b[1;32m     19\u001b[0m net\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:699\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    697\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 699\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    701\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    702\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    703\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    704\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 230\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'epoch_1.pth'"]}],"source":["if __name__ == '__main__':\n","    main()"]},{"cell_type":"markdown","metadata":{"id":"TckxFyr3Nl8r"},"source":["###**콘텐츠 라이선스**\n","\n","<font color='red'><b>**WARNING**</b></font> : **본 교육 콘텐츠의 지식재산권은 재단법인 네이버커넥트에 귀속됩니다. 본 콘텐츠를 어떠한 경로로든 외부로 유출 및 수정하는 행위를 엄격히 금합니다.** 다만, 비영리적 교육 및 연구활동에 한정되어 사용할 수 있으나 재단의 허락을 받아야 합니다. 이를 위반하는 경우, 관련 법률에 따라 책임을 질 수 있습니다.\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
